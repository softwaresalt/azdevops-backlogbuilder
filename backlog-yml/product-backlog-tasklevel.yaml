features:
  - title: "Flight Segment Data Acquisition & On-Board Model Hosting"
    description: >
      Establish a robust on-board infrastructure for collecting instrument data, routing telemetry, and hosting containerized ML models for autonomous science, ensuring offline capability and seamless integration with the flight computer.
    user_stories:
      - title: "Implement and Configure On-Board Data Bus (SpaceWire) for High Availability"
        description: >
          As an FSW Engineer, I want to implement and configure the SpaceWire data bus service on our redundant flight computers to ensure high availability and prevent a single point of failure for our on-board data pipeline.
        acceptance_criteria:
          - "Redundant SpaceWire interfaces (Side A / Side B) are operational on the flight computers."
          - "C&DH FSW is configured for automatic failover between redundant computers."
          - "Performance is benchmarked to handle the expected data throughput from all instruments."
          - "Configuration is managed via the FSW build process and stored in the GitHub repository."
      - title: "Define and Implement CCSDS Packet Topic Structure"
        description: >
          As a Systems Engineer, I want to design and implement a standardized, hierarchical packet structure (based on the CCSDS standard) within the FSW, so that all telemetry data is organized, addressable, and context-rich.
        acceptance_criteria:
          - "A document defining the telemetry packet hierarchy (APIDs) is created and approved."
          - "The C&DH FSW is configured to route packets based on this structure."
          - "Example data from the radar instrument is published to the bus using the correct packet format."
      - title: "Configure Instrument Interfaces for Data Publishing"
        description: >
          As an Instrument Engineer, I want to configure the Ice-Penetrating Radar and Mass Spectrometer to connect to the C&DH, model the relevant data points, and publish them to the SpaceWire bus under the correct CCSDS packet definition.
        acceptance_criteria:
          - "A secure connection is established between the radar/mass spec and the C&DH."
          - "Data models for the instrument science tags are created in the FSW."
          - "Instruments are configured to publish data to the SpaceWire bus, mapping to the defined packet structure."
          - "Data flow is validated from both instruments to the C&DH."
      - title: "Install and Configure Flight Executive (cFE/CFS)"
        description: >
          As an FSW Engineer, I want to install and configure the Core Flight Executive (cFE) on our RAD750 flight computer, so we have a resilient and scalable platform for running our FSW modules.
        acceptance_criteria:
          - "A multi-module cFE/CFS instance is installed and configured on the RAD750."
          - "The instance has command validation enabled."
          - "The ground system is configured as a trusted source for new FSW modules."
      - title: "Install and Register AMO on the Flight Computer"
        description: >
          As an FSW Engineer, I want to install the Autonomous Mission Operations (AMO) service on our on-board flight computer and register it with our MOC, so that it can be managed and receive deployment instructions from the ground.
        acceptance_criteria:
          - "The AMO FSW module is successfully deployed to the flight computer."
          - "The AMO service is successfully registered in the MOC and shows a connected state."
          - "The 'amo' command-line tool can be used to check the status of the runtime and modules."
      - title: "Deploy and Validate the ML Model Module On-Flight"
        description: >
          As an MLOps Engineer, I want to perform the first deployment of the ML model module to the on-board computer via the CD pipeline and run a smoke test to validate that it's running correctly.
        acceptance_criteria:
          - "The CD pipeline successfully deploys the model module to the flight computer."
          - "The model module starts and is in a 'Running' state."
          - "The module telemetry shows a successful subscription to the data bus."
          - "A test message sent to the input APID results in a valid prediction on the output APID."

  - title: "Secure Ground Data Ingestion"
    description: >
      Implement robust mechanisms for securely downlinking telemetry from the spacecraft to the Deep Space Network (DSN) and Ground Data System (GDS), ensuring data integrity, confidentiality, and efficient real-time processing.
    user_stories:
      - title: "Establish Secure Downlink from C&DH to DSN"
        description: >
          As a Downlink Engineer, I want to configure the C&DH data buffer to securely stream data to the Deep Space Network (DSN) receiver, so that data is reliably transferred from flight to ground.
        acceptance_criteria:
          - "The FSW Downlink Extension is configured on the on-board C&DH."
          - "The connection uses appropriate space-ground protocols for end-to-end validation."
          - "Data from specific APIDs is successfully downlinked to the target GDS."
      - title: "Implement and Test On-Board Data Buffering"
        description: >
          As an FSW Engineer, I want to configure and test the buffering mechanism on the C&DH downlink service to ensure no data is lost during a simulated communications blackout.
        acceptance_criteria:
          - "The C&DH service is configured with a solid-state buffer of a specified size."
          - "A test is conducted where the DSN connection is severed for a period."
          - "Upon reconnection, the C&DH successfully sends all buffered data to the GDS."
          - "No data loss is verified by comparing on-board and ground data counts."
      - title: "Provision and Configure GDS for Scalability"
        description: >
          As a Ground Segment Engineer, I want to provision a GDS ingest pipeline with a partition strategy that supports high-throughput and parallel consumption, so that our data ingestion pipeline is scalable and performant.
        acceptance_criteria:
          - "A GDS ingest point is provisioned using Infrastructure as Code (Terraform)."
          - "The pipeline is configured with an appropriate number of partitions based on expected data volume."
          - "A dedicated consumer group is created for the SDPP streaming job."
      - title: "Develop Level 0 Ingestion Processing Job"
        description: >
          As a Data Engineer, I want to develop a SDPP Spark streaming job that reads raw data from the GDS and writes it to the Level 0 PDS Archive as immutable FITS files, partitioned by date.
        acceptance_criteria:
          - "The Spark job uses the GDS connector to read the data stream."
          - "The job writes the raw event payload (including metadata) to the PDS in FITS format."
          - "Data is partitioned by year, month, and day for efficient querying."
          - "The job uses watermarking to handle late-arriving data and manages checkpoints."
      - title: "Implement a Bad Telemetry Queue for Ingestion Errors"
        description: >
          As a Data Engineer, I want to implement a bad-telemetry mechanism for the Level 0 ingestion job, so that any corrupted or un-processable frames are captured for later analysis without halting the entire stream.
        acceptance_criteria:
          - "The Spark streaming job includes a try/except block to catch parsing errors."
          - "Frames that fail to process are written to a separate 'quarantine' directory in the PDS."
          - "An alert is configured to notify the data engineering team when new messages land."

  - title: "GDS Data Integration & Curation (Science Levels)"
    description: >
      Establish a scalable and structured data environment within the GDS using a science-level architecture (Level 0, 2, 3) to ingest, clean, calibrate, enrich, and model data for advanced analytics and ML operations.
    user_stories:
      - title: "Develop Level 2 Cleansing and Calibration Job"
        description: >
          As a Data Engineer, I want to develop a job that processes raw Level 0 data, applies calibration files, and conforms it into a well-defined schema in the Level 2 layer of the Science Database.
        acceptance_criteria:
          - "The job reads new data from the Level 0 layer."
          - "Data is deduplicated based on a unique packet identifier."
          - "Data types are enforced, and null values are handled."
          - "The cleaned and calibrated data is written to Delta tables in the Level 2 layer."
      - title: "Develop Level 3 Aggregation Job for Analytics"
        description: >
          As a Science Analyst, I want to create a data pipeline that aggregates Level 2 layer data into science-centric models (e.g., daily plume activity maps) in the Level 3 layer, optimized for reporting.
        acceptance_criteria:
          - "A pipeline or notebook reads from Level 2 layer Delta tables."
          - "Data is aggregated into key metrics over specific time windows."
          - "The aggregated data is stored in Level 3 layer Delta tables."
          - "The resulting tables are performant for queries from analytics tools."
      - title: "Integrate Mission Scheduling Data into SOC"
        description: >
          As a BI Developer, I want to use a connector in the SOC to ingest DSN scheduling data (e.g., comms windows, data rates) into the Level 3 layer of the PDS, so it can be joined with our FSW data.
        acceptance_criteria:
          - "A connection to the MOC's DSN scheduling system is established in the SOC."
          - "A dataflow or pipeline is created to incrementally load relevant schedule tables."
          - "The data is available as Delta tables and can be queried."

  - title: "Mission Reporting & Visualization"
    description: >
      Implement a structured deployment pipeline for BI assets (semantic models, dashboards, reports, apps) across Dev, Test, and Production environments, enabling high-performance analytics and broad user consumption.
    user_stories:
      - title: "Set Up BI Deployment Pipelines for Environments"
        description: >
          As a BI developer, I want to establish a structured deployment pipeline spanning Dev, Test, and Prod workspaces for BI assets, so that changes can be promoted consistently and reliably.
        acceptance_criteria:
          - "Dev, Test, and Production BI workspaces are created and configured."
          - "A BI deployment pipeline is set up to connect these workspaces."
          - "Changes to BI assets can be successfully promoted from Dev to Test, and from Test to Production."
      - title: "Author and Test Semantic Models in Development Workspace"
        description: >
          As a BI developer, I want to author and test semantic models in the Dev workspace, so that data definitions are consistent and accurate before wider consumption.
        acceptance_criteria:
          - "Semantic models are developed in the Dev workspace, connecting to Level 3 data."
          - "Data definitions within the semantic models are consistent and accurate."
          - "Semantic models are thoroughly tested in the Dev environment."
      - title: "Conduct Validation of BI Assets in Test Workspace"
        description: >
          As a BI developer, I want to validate semantic models, dashboards, and reports in the Test workspace, so that functionality and data accuracy can be confirmed by a limited user group.
        acceptance_criteria:
          - "BI assets are successfully deployed to the Test workspace via the deployment pipeline."
          - "A limited group of users (e.g., UAT testers) can access and review the assets."
          - "Functionality and data accuracy are validated by the test user group."
      - title: "Deploy Approved BI Content to Production"
        description: >
          As a BI developer, I want to deploy approved semantic models, dashboards, reports, and apps to the Production workspace, so that end-users have access to the final, verified content.
        acceptance_criteria:
          - "Approved BI assets are successfully deployed to the Production workspace."
          - "All semantic models, dashboards, reports, and apps are accessible."
          - "End-users can access the final, verified content without issues."
      - title: "Enable Direct Lake Connection for Production BI Reports"
        description: >
          As a BI developer, I want to connect production BI reports and dashboards directly to Level 3 datasets via Direct Lake, so that high-performance analytics are achieved with minimal data movement.
        acceptance_criteria:
          - "BI reports and dashboards are configured to connect to Level 3 datasets using Direct Lake mode."
          - "Query performance of reports and dashboards meets high-performance requirements."
          - "Insights displayed in reports are consistently up-to-date with the Level 3 layer data."
      - title: "Maintain Consistent Semantic Models Across Environments"
        description: >
          As a BI lead, I want to maintain a single semantic model per environment and propagate it, so that data consistency and governance are ensured across all reporting.
        acceptance_criteria:
          - "A single, canonical semantic model is defined and used for each environment."
          - "Changes to the semantic model are propagated consistently through the deployment pipeline."
          - "The semantic model serves as a single source of truth for all reports."
      - title: "Package and Publish BI Apps for User Access"
        description: >
          As a BI developer, I want to support robust application packaging and publishing across workspaces using Fabric apps, so that end-users can easily access relevant reports.
        acceptance_criteria:
          - "BI reports and dashboards are grouped into Fabric apps."
          - "Apps are published to relevant end-user groups with appropriate roles and permissions."
          - "End-users can easily discover and access the published BI apps."
      - title: "Create BI Reports for Flight Operations Performance"
        description: >
          As a Flight Controller, I want BI reports that utilize summarized telemetry data to show real-time spacecraft health and key operational metrics, so that I can monitor status and identify anomalies.
        acceptance_criteria:
          - "BI reports are developed that display summarized telemetry data."
          - "Key operational metrics (e.g., data volume, buffer status, system temps) are accurately presented."
          - "Reports provide near real-time insights into spacecraft health."
      - title: "Implement BI Reports for FSW Model Performance Monitoring"
        description: >
          As a Data Scientist, I want BI reports that track FSW model performance, including data drift and key outputs, so that I can ensure the models are performing as expected and identify needs for retraining.
        acceptance_criteria:
          - "BI reports are developed to track key FSW model performance metrics."
          - "Reports clearly visualize data drift detected in model inputs over time."
          - "Key outputs and predictions from FSW models are monitored and displayed."
          - "Reports provide clear indicators or triggers for when model retraining might be necessary."

  - title: "MLOps & FSW Uplink Lifecycle Management"
    description: >
      Establish comprehensive MLOps practices within the SOC to manage the entire ML model lifecycle, from training and versioning to cross-compilation, uplink, monitoring, and retraining.
    user_stories:
      - title: "Use GitHub Enterprise for FSW and Model Source Control"
        description: >
          As an MLOps Engineer, I want to use GitHub Enterprise for robust source control of ML models, notebooks, and FSW modules, so that versioning is tracked and collaboration is streamlined.
        acceptance_criteria:
          - "All ML models, notebooks, and FSW C++ code are stored in GitHub Enterprise."
          - "Versioning for all assets is properly tracked and accessible through GitHub."
          - "Collaboration features (e.g., branching, pull requests) are utilized."
      - title: "Employ MLFlow for ML Experiment and Version Tracking"
        description: >
          As an ML engineer, I want to employ integrated MLFlow within the SOC to track ML experiments and manage model versions, so that model development is organized and reproducible.
        acceptance_criteria:
          - "MLFlow is successfully integrated and configured within the SOC."
          - "All ML experiments (parameters, metrics, artifacts) are tracked using MLFlow."
          - "Different model versions are managed and visible within MLFlow."
          - "Model development is reproducible based on MLFlow tracking data."
      - title: "Develop a Production-Ready FSW Inference Module"
        description: >
          As an FSW Engineer, I want to develop a lightweight C/C++ module that loads the trained model and exposes an FSW API for inference, so the model can be easily consumed by the flight software.
        acceptance_criteria:
          - "A C/C++ module for the FSW is created."
          - "The module includes a 'predict()' function that accepts input data and returns a prediction."
          - "The module includes a 'health_check()' function for health checks."
      - title: "Implement CI Pipeline to Build and Archive FSW Module"
        description: >
          As an MLOps Engineer, I want to create a GitHub Actions CI workflow that automatically cross-compiles, tags, and pushes the model's FSW binary to Artifactory when a new model is tagged for release.
        acceptance_criteria:
          - "A GitHub Actions workflow file is created."
          - "The workflow is triggered by a new Git tag (e.g., 'v1.2.0')."
          - "The workflow authenticates to Artifactory."
          - "The FSW binary is built and pushed to Artifactory with a tag."
      - title: "Implement CD Pipeline to Uplink Module via AMO"
        description: >
          As an MLOps Engineer, I want to extend the GitHub Actions workflow to act as a CD pipeline that, after a successful binary push, updates the Autonomous Mission Operations (AMO) sequence file to roll out the new FSW module.
        acceptance_criteria:
          - "The workflow retrieves the AMO sequence file from the repository."
          - "It updates the module version in the sequence."
          - "It uses a CLI to apply the updated sequence to the MOC uplink queue."
          - "The pipeline includes a manual approval step from Mission Ops before uplink."
      - title: "Establish FSW Model Monitoring and Retraining Procedures"
        description: >
          As a data scientist, I want to establish practices for monitoring FSW model performance (e.g., data drift, output validation) and defining triggers for retraining, so that models remain accurate.
        acceptance_criteria:
          - "Monitoring mechanisms are in place to track FSW model performance metrics."
          - "Data drift detection is implemented and monitored for model inputs."
          - "Model output validation procedures are established."
          - "Clear triggers and criteria for model retraining are defined."

  - title: "Mission Security & Governance"
    description: >
      Implement a robust security and governance framework across the GDS and flight segment interfaces, ensuring centralized monitoring, strong identity and access management, compliance, and proactive threat detection.
    user_stories:
      - title: "Consolidate Mission SOC for Multi-Center Monitoring"
        description: >
          As a security architect, I want to centralize monitoring using a consolidated Mission SOC (Sentinel) instance across MOC and SOC tenants, so that security visibility is enhanced.
        acceptance_criteria:
          - "A consolidated Sentinel instance is deployed and configured to monitor both MOC and SOC tenants."
          - "Security logs and events from both tenants are successfully ingested."
          - "Security visibility across both tenants is enhanced via centralized dashboards and alerts."
      - title: "Implement Strong Identity and Access Management"
        description: >
          As a security architect, I want to implement NASA Identity Services (Entra ID) with conditional access and multifactor authentication (MFA), so that robust identity and access management is enforced.
        acceptance_criteria:
          - "NASA Identity Services is configured as the primary identity provider."
          - "Conditional Access policies are defined and enforced."
          - "MFA is enabled and enforced for all relevant user groups."
          - "Only authorized users can access resources."
      - title: "Ensure Comprehensive Auditing, Logging, and Compliance"
        description: >
          As a security architect, I want to establish thorough auditing, logging (including sign-in logs to Log Analytics), and compliance monitoring practices, so that security incidents can be investigated.
        acceptance_criteria:
          - "Comprehensive auditing is enabled across all critical GDS resources."
          - "All relevant logs, including sign-in logs, are successfully ingested into a Log Analytics workspace."
          - "Compliance monitoring tools are configured to track adherence to NASA standards."
      - title: "Enforce Ground Segment Policy for Consistent Resource Deployment"
        description: >
          As an IT administrator, I want to enforce GDS Policy for resource configuration, so that resources are deployed with consistent security baselines and comply with organizational standards.
        acceptance_criteria:
          - "GDS Policy definitions are created and assigned to enforce desired resource configurations."
          - "All new resource deployments automatically comply with the defined policies."
          - "Non-compliant resources are identified and remediated."
      - title: "Utilize Secure Credential Storage for Ground Services"
        description: >
          As a security architect, I want to centralize the storage of credentials, passwords, and certificates in a secure vault (e.g., Key Vault), so that sensitive information is securely managed.
        acceptance_criteria:
          - "All specified credentials, passwords, and certificates are securely stored in a vault."
          - "Access policies are configured to ensure only authorized services can retrieve secrets."
          - "Sensitive information is successfully managed and accessed through the vault."
      - title: "Implement Automated Secrets Rotation Policy"
        description: >
          As a security architect, I want to implement and automate a rotation policy for secrets stored in the vault, so that the risk of credential compromise is minimized over time.
        acceptance_criteria:
          - "A secrets rotation policy is defined for all relevant secrets."
          - "Automated processes are implemented to execute the secrets rotation policy."
          - "Secrets are automatically rotated according to the defined schedule."
      - title: "Govern Cross-Center Access for MOC/SOC Guest Accounts"
        description: >
          As an IT administrator, I want to effectively manage cross-tenant access for MOC resources accessing the SOC tenant via guest accounts and cross-tenant synchronization (Lighthouse), so that collaboration is enabled with control.
        acceptance_criteria:
          - "Cross-tenant access for MOC resources is enabled via guest accounts in the SOC tenant."
          - "Azure Lighthouse is configured for cross-tenant synchronization."
          - "Access for MOC users is granted with the principle of least privilege."
      - title: "Conduct Periodic Access Reviews for Guest Accounts"
        description: >
          As a security administrator, I want to conduct regular access reviews for guest accounts, including setting expiration dates and using automation for stale accounts, so that unnecessary access is revoked.
        acceptance_criteria:
          - "A process for regular access reviews of all guest accounts is established."
          - "Expiration dates are set for guest accounts as appropriate."
          - "Automation scripts are executed to identify and manage stale guest accounts."
      - title: "Encrypt and Authenticate Telemetry Downlink"
        description: >
          As a network engineer, I want to secure communications between the DSN and GDS using TLS 1.2/1.3 and implement client authentication, so that data integrity and confidentiality are maintained.
        acceptance_criteria:
          - "All communications between DSN and GDS utilize TLS 1.2/1.3 encryption."
          - "Client authentication mechanisms are implemented and enforced."
          - "Only authenticated ground stations can establish connections."
      - title: "Configure Firewalls and VNet Peering for Secure Connectivity"
        description: >
          As a network engineer, I want to review and configure firewall rules and VNet peering between MOC and SOC environments, so that network traffic is managed securely.
        acceptance_criteria:
          - "Firewall rules are reviewed and configured to allow only necessary traffic."
          - "VNet peering is successfully established between relevant VNets."
          - "Network traffic flows securely and as intended between all connected environments."

  - title: "Mission Operations Monitoring & Analytics"
    description: >
      Implement comprehensive monitoring, alerting, and performance tracking across the GDS, including real-time dashboards, FSW model performance monitoring, and detailed logging of uplink deployments.
    user_stories:
      - title: "Create BI Dashboards for Real-time Flight Operations"
        description: >
          As a Flight Controller, I want real-time operational dashboards in the MOC, so that I can immediately visualize spacecraft health and performance metrics.
        acceptance_criteria:
          - "BI dashboards are developed and display real-time telemetry."
          - "Dashboards are accessible to flight controllers."
          - "Operators can immediately visualize key performance indicators (KPIs)."
          - "The dashboards effectively highlight anomalies or issues."
      - title: "Set Up Automated Alerts for Critical Telemetry and Model Outputs"
        description: >
          As a Flight Director, I want to configure alerts based on critical telemetry readings and FSW model outputs, so that immediate notifications (e.g., via Teams or email) are sent when thresholds are breached.
        acceptance_criteria:
          - "Alerting mechanisms are configured for critical telemetry (e.g., system temps, power)."
          - "Alerting mechanisms are configured for critical FSW model outputs (e.g., plume detected)."
          - "Notifications are sent automatically when predefined thresholds are breached."
      - title: "Collect Flight Telemetry and Health Data in GDS"
        description: >
          As an IT administrator, I want to integrate Log Analytics for comprehensive telemetry, FSW model performance monitoring, and health assessments of on-board systems, so that issues can be diagnosed.
        acceptance_criteria:
          - "Flight telemetry data is successfully collected and ingested into Log Analytics."
          - "FSW model performance metrics are monitored in Log Analytics."
          - "Health assessments of on-board systems are visible within Log Analytics."
      - title: "Diagnose and Resolve GDS Pipeline Failures"
        description: >
          As a data engineer, I want to identify and resolve issues causing random failures and excessive idle time in GDS processing notebooks, so that daily data processing jobs run reliably.
        acceptance_criteria:
          - "A process for identifying root causes of random failures is established."
          - "Mechanisms are in place to detect and address excessive idle time."
          - "Solutions are implemented to resolve identified issues."
          - "Daily science data processing jobs run reliably."
      - title: "Restore Functionality of GDS Capacity Metrics Dashboard"
        description: >
          As a data engineer, I want the GDS capacity metrics dashboard to be fixed, so that resource utilization can be accurately monitored, capacity planning is informed, and costs can be managed.
        acceptance_criteria:
          - "The GDS capacity metrics dashboard is fully functional."
          - "Resource utilization is accurately displayed on the dashboard."
          - "The dashboard provides sufficient data to inform capacity planning decisions."

  - title: "External Partner Data Exchange"
    description: >
      Establish secure and efficient mechanisms for data sharing and collaboration with external partners (e.g., University SOC, SwRI), primarily through SFTP, with consideration for alternative programmatic solutions.
    user_stories:
      - title: "Configure GDS Pipelines for Partner SFTP Data Transfers"
        description: >
          As a data engineer, I want to utilize GDS pipelines (e.g., ADF) for secure data transfers to external SFTP servers, so that efficient partner data sharing and collaboration is enabled.
        acceptance_criteria:
          - "GDS pipelines are configured to connect to specified external SFTP servers."
          - "Data is successfully transferred to and from SFTP servers."
          - "Data transfers are secure (e.g., using SSH keys or secure authentication)."
          - "Data transfer efficiency meets defined performance requirements."
      - title: "Securely Manage Partner SFTP/API Credentials"
        description: >
          As a security architect, I want to ensure that credentials (usernames, passwords, certificates) used for partner SFTP and API integrations are securely stored (in Key Vault) and managed, so that sensitive access information is protected.
        acceptance_criteria:
          - "All partner SFTP and API credentials are stored securely in Key Vault."
          - "Access controls are implemented to ensure only authorized services can retrieve credentials."
          - "Credential management adheres to best practices."
      - title: "Assess Data Sensitivity for External Partner Transfers"
        description: >
          As a data owner, I want to assess the sensitivity of data (e.g., Level 0 data, calibration files) being transferred to external partners, so that appropriate security measures are applied.
        acceptance_criteria:
          - "A process for assessing data sensitivity for external transfers is defined."
          - "Data sensitivity levels are assigned to all relevant data types."
          - "Appropriate security measures (e.g., PGP encryption) are applied based on sensitivity."
      - title: "Evaluate C# Function App as SFTP Transfer Alternative"
        description: >
          As a data engineer, I want to evaluate the use of a C# function app with a timer trigger for SFTP transfers as an alternative to GDS pipelines, so that a more lightweight solution is available.
        acceptance_criteria:
          - "A C# function app is developed to perform SFTP transfers."
          - "The function app is configured with a timer trigger for scheduled transfers."
          - "The C# function app's performance and reliability are evaluated against GDS pipelines."
          - "Documentation is provided on when a C# function app is a more suitable alternative."
