Fabric-Architecture Workshop Meeting Recording
2h 0m 57s

Participant_A   0:03
OK.

Participant_B   0:05
There you go. It's better that I start it anyway because then I can download it and do everything. So it's better that I start it anyway. So you're all set.

Participant_A   0:07
R.
OK. Uh, thanks everybody for joining and um, I think.
One of the things we wanted to kick off today in order to make good use of everybody's time is to start the call with source control and.
Yeah, like Azure DevOps, source control, Azure DevOps being sort of a shorthand for we got one more person to admit shorthand for process management. So like, you know, user stories, features, ethics, tasks, these kinds of things.
So with that, I wanted to get that started because I know that's gonna, that's gonna touch people doing ML models, that's gonna touch people doing RBI, that's gonna touch everybody. So and then once we're done with that, folks who are doing just BI and we're just gonna.
Going to be getting into like MLOPS and stuff. The folks in Power BI can probably they're free to drop off and do other things. Does that sound good?

Participant_R   1:28
Yep.

Participant_A   1:30
Sweet. OK, so um.
Consistent with last with last week's way of of running the workshop, the idea is to get you to sort of talk as much as possible. So why don't somebody, I don't know who's the right person, but can somebody is kind of.
Cover their their thinking around source control and workflow process management.

Participant_D   2:04
Yeah, like what? What do you guys use today for for deploying builds out?

Participant_P   2:14
Guess I can jump in and sort of answer the question. Feel free to.
Expand on my answer, Participant_Q or Participant_E, but currently for Power BI reports we aren't doing source control. I'm personally not doing source control for notebooks or anything.
Really. Other than Data Factory? Um.
Pipelines. I think that's the the only thing we've set up, at least at Acme-Sub in for.

Participant_Q   2:51
Right. No, it's kind of the same thing too with with that and that's some of the operational maturity that we need to kind of get around is that we do that for our Data Factory pipelines as well. We use DevOps repo, but to Participant_P's point, we we don't.
You know, it's just, you know, essentially a letter buck, you know, and and and we just, you know, develop and and do it. If it doesn't work, go back and do it again. So that would be one thing that we'd want to probably, you know, wrangle in a little bit better.

Participant_D   3:27
Do you guys use, do you guys use DevOps like anywhere else around the organization, not just limited to the, you know, the Fabric solution?

Participant_Q   3:39
Oh, for yeah, for sure. We we have, we have development pipelines within DevOps for our Scout app, but everything else is just otherwise a, you know, like the CICD. The other other pieces are just kind of a, you know, source control.

Participant_D   3:56
Uh.

Participant_Q   3:59
And anybody jump in there, you've got any additional information for sure?

Participant_A   3:59
What do you mean by?

Participant_D   4:00
It.

Participant_A   4:04
You said otherwise, you know, like the just the source control you said. So could you elaborate on that? Are you using any kind of source control?

Participant_Q   4:12
Well, yeah, well within DevOps we have the, the, the, you know the repositories within within that and then we you know do pull requests and that that's the source control that we use. You know we have to have reviewers on you know, but it it is really limited to at this point data.
Factory and then there's some other development that we use for our scout app, which which actually is.
A true CIC pipeline that'll actually push and and do, you know, bring it through the various stages.

Participant_D   4:47
OK. And then the the ADO instance, is it, is it, is it bound to the ACME tenant or the idle one?

Participant_Q   4:58
We have.
Participant_P, why don't you, why don't you speak to that? I think we we do have some, we've they've got their own tenant, but a lot of the 80, the the Data Factory pipelines are in our tenant and so that's bound to ours.
But what's your understanding on that, Participant_E Participant_P?

Participant_P   5:22
All of the pipelines I build are in ACMES tenant, so I'm not really familiarized with what's on I I I do know we do have a couple of them, but it's like 3-4 percent of them. It's not a big number.

Participant_Q   5:28
Yeah.

Participant_E   5:39
Yep, that's accurate. The only, sorry, the only ADF that we have that we're using in Acme-Sub and tenant is actually for like Power BI API work or whatever. I think we're just using the managed identity because it's in the same tenant.

Participant_Q   5:40
Yeah. So primarily it's us, yeah.

Participant_E   5:59
As our Power BI tenant is, but the workloads that are not relying on Identity Management or providing managed identity as the auth method, we just use the ACME resources for that.
And the the quick explanation of how and why is just our BI team started with one person about four years ago. So it was it didn't make sense to build out everything into a tenant we kind of hopped into with ACME and then.
Things that can't be done in their tenant, we we do in our tenant. So that's the quick of just why and how.

Participant_D   6:43
Yeah.
OK. And then um.

Participant_P   6:45
Back.

Participant_D   6:49
Like within your within your your current ADO instance, do you have any security extensions enabled that's that's looking at like static or dynamic code analysis like cred band etcetera?
Do you know of?

Participant_M   7:09
No.

Participant_D   7:10
OK.

Participant_M   7:12
We've got some stuff in GitHub Enterprise, um, so I think some of that's in there, but as far as plugged into those pipelines, not to my knowledge.

Participant_A   7:20
Yeah, so tell us, tell us more about you've got. It sounds like you are using.

Participant_D   7:21
OK.

Participant_A   7:30
Yeah, I just hear you say GitHub Enterprise. So it sounds like you're used. There's like you got one version control system for this and a different source control system for for that. What are you using GitHub Enterprise for?

Participant_C   7:49
OK, let's scratch all. Let's scratch that, right? Let's ask the question. Let's ask this question. What is your future plan? Where are you, you know, going to see yourself? Are you going to use Azure DevOps or the plan is to use Git Enterprise?

Participant_G   7:50
That's where.
So sorry.

Participant_C   8:05
Right. Yes, go ahead.

Participant_G   8:05
Now the plan is to is to we are licensed on Git Enterprise and we've got quite a few folks utilizing that and several repos out there for for future state. Current state on in development is is DevOps.

Participant_C   8:11
OK.

Participant_G   8:20
But they want to migrate those as time permits.

Participant_C   8:26
OK, that makes sense.

Participant_G   8:28
Yep.

Participant_C   8:30
OK.

Participant_A   8:32
OK.

Participant_C   8:33
Because, you know, that's what I would recommend, right? Have the source control for Fabric, right? Integrate with the Git, right? Plus the Power BI and any kind of manual exports, right? And then the the YAML pipelines and the Race APIs, everything, right?

Participant_G   8:38
Mhm.
Right.
Yep.

Participant_C   8:50
And then, you know, obviously the the notebooks or the Pi notebooks, right, we should actually all track that into into Git as well. That's what you know, we would definitely recommend. Participant_A, do you agree with that?

Participant_G   9:04
Yeah, a lot of those notebooks are. I mean from, I don't know if Drew is on, but he keeps those that work in a repo and a lot of other. I mean from the DBA standpoint, I know they put a lot of things into Git Enterprise as well.

Participant_A   9:05
I mean.

Participant_G   9:21
But we're utilizing the the Enterprise Copilot within Git.
GitHub as well, and because that's where we wanted to prototype a lot of that work in there.

Participant_C   9:36
Yep, Yep. At the moment, do you guys use any kind of branching strategies or you haven't started that process yet?

Participant_G   9:46
If minimal.

Participant_C   9:48
That's minimum. OK, so pretty much at this second right the current state is you know if I if I may use the word trial and error or very basic is is the right term. I I maybe don't want to offend anyone seems very yeah.

Participant_G   9:49
Yeah.
Yeah.

Participant_Q   10:02
No, no. And I think you're right. I mean we do, you know we'll do it for branching. We'll just do a user and then project and then you know that's that's how that's how we do it. But open to you know, no pride there any any suggestions are great.

Participant_C   10:07
Mm-hmm.
Get.
No, definitely. That's the plan here, right? So you know, obviously we need to make sure, right, if we have branches like dev, QA and main, we need to protect the branches with the PRS. Obviously the, you know, the PR should be reviewed by someone.
And then once that gets approved, then it should be pushed to the appropriate, you know, branching like a dev or a QA or or or main, whatever that may be. So yeah, definitely, yeah.

Participant_Q   10:49
Right. Yeah, 'cause we we don't do that now. It's just one one branch based on user and then you, you know, complete the merge and that that's that's where we're at currently.

Participant_C   10:55
Mhm.
OK.

Participant_A   11:02
I mean, yeah, for a team this size, that can totally work.
I think.

Participant_G   11:07
Yeah, I mean from an analytics standpoint, most of the team, if not, yeah, I would say most of the team is does not come from an IT background. And so a lot of the a lot of this is is on the maturity scale. We live and learn here a bit.

Participant_A   11:21
Yeah, yeah, that totally makes sense. Yeah, an ADO can as a process tool can hook in to GitHub. I mean, it can use GitHub or you can use its own native git, you know, repo, but.

Participant_G   11:33
Mhm.
Yep.

Participant_A   11:37
It's yeah, it's interchangeable. All right. Sorry as I know. So that's that's totally that works.

Participant_G   11:39
Yep.
The the migration should be a lot easier for feedback for CloudProviderA that the development guys would love to see that the migration be a lot easier.

Participant_A   11:56
Um, let's see. OK, so we covered.
And then I guess what we haven't really covered are branching strategies so much. I know Participant_C referred to it, but there are, you know, concepts of like there's monorepos and then there are.
Trying to break up repos for separate things. There's arguments for different things. What's? What are your thoughts on this?
Or do you have any?

Participant_G   12:30
I guess historically we've typically gone. I mean, in my background, it was all fairly straightforward. I mean, I know there's different approaches, but.
The methodology was generally the simpler, the better.
Just because, yeah, to all avoid conflict, right? And then, but preserve, yeah.

Participant_Q   12:51
Have the flexibility too.

Participant_A   12:57
I think what's simple can be relative to how you how you deploy things.

Participant_G   13:02
Yeah, for sure.

Participant_A   13:06
So, so I'm I suspect that what we want to do is really dive into like how you foresee deploying things and that will kind of help shape our what kind of repo strategy we we we want to have.

Participant_G   13:21
Yeah, I guess I've done it both ways where we've got defined, defined deployment or release date and all of the development needs to be done within our defined timeline. But I mean from and then I've also done more of a.
Plays, you know, checking as you go and and it's always.
Basically live.
Checking into, yeah, you've done done both ways.

Participant_D   13:48
So.
So, so within the current process, Participant_G like you know as the like let's say for example the Power Power BI developers are are developing, are the Power BI developers outside of the core team? Are they like on a separate team or?

Participant_A   13:53
Yeah.

Participant_G   14:02
Mm-hmm.
I mean.
No, they're they're we're you know same team, but I mean the the Power BI, I mean the the Power BI workbooks themselves are typically just stored in OneDrive as as the backup, you know original.

Participant_D   14:13
OK.

Participant_G   14:25
But then published to the the workspaces in the service and so then you've got a built in backup on the one the OneDrive with the with the you know, the workbook itself and you know.

Participant_D   14:36
Mhm.
Yeah, yeah. So how do they like when when when they have a build do they do they currently deploy to a like a non production environment to validate the you know the solution or like is there like how is there a formal process around that cause like?

Participant_A   14:40
Right. So I.

Participant_D   14:55
As you mentioned, it's probably very basic, right? You know, implementation, you know to what to what level are you guys, are you guys, you know where I'm trying to understand is the like to what level is the maturity within the within the CICD process, right, the the relief strategy.

Participant_G   14:59
Yep.
From a from a DBA team standpoint, I mean those are, those are just are developed and reviewed and released. Isn't that right Participant_Q, I mean and Participant_O?

Participant_D   15:22
OK.

Participant_G   15:26
I don't know who else is on.

Participant_Q   15:26
Yeah, yeah. There was a question about security and and things and and yeah, since we and that's what I was answering maybe we just kind of do it, you know, try to do groups, we try to do you know at the workspace level and then or or at the schema level in the data warehouse.
You know, based on what what we need, every once in a while there's a there's a an instance where it's a single and that happens a lot of things with guest accounts where we have to set the security. Is that answering the question or am I off because I was, I was kind of focused on the question that Participant_C had asked.

Participant_D   15:53
OK.
No, I think this is good. This is good because it's leading into some of the discussion points, right. So you mentioned the the workspaces, right? Like you know and then obviously at at the data, at the data layer, how is authorization managed, right, which you know is also RBAC.

Participant_Q   16:19
We we are in the process of of kind of figuring that out from a, you know, an overall governance standpoint as far as how do you request access to not only to a resource but to create a resource.
And so that's another part of the journey that we have to do is is we're we're figuring that out. There's you know have an opportunity to sit on the team and that's going to figure that out and right it's it honestly guys it's Wild West it was you know.

Participant_D   16:53
Yeah.

Participant_Q   16:53
Until until we decided to try to wrap our arms around it. So you know the the short the shorter answer after I make a long one is that you know we just whoever's got contributor access can go ahead and you know let it fly. But we want to try to not.
Maybe not, you know, just have some governance around that.

Participant_D   17:12
Yeah, yeah. I mean this is, yeah, I mean obviously we'll we'll be making recommendations and then in future calls we can, you know, go down to you know, you know, various reference architectures that we use, right. Like for example, you can potentially use identity governance to control access to various resources.
What this does is, is that it provides you the ability to kind of put governance around, you know, you know, entitlement management. An example would be, you know, like you could, you could have people request, request access, which then you know it goes through a whole life cycle or approval process.

Participant_Q   17:43
Oh, right.

Participant_D   17:52
US right to ensure OK.

Participant_Q   17:52
And I guess we do, we do a little of that. Participant_N, can you speak to that a little bit because we do have some, you know, we call them CSA accounts, but it's basically privileged identity management that we do have.

Participant_N   17:57
Mhm.
Yeah, this is more access packages I think he's referring to and I don't, I don't know there's there's kind of a confusion on our end on like where you know what's the threshold of requiring an identity governance license because we're only intro P2 license at the at the time.

Participant_D   18:08
Yeah.

Participant_N   18:22
Time.

Participant_D   18:23
No worries. Yeah, we'll we'll we'll take, I'm noting that and then you know we could come back to that in a lot more detail. So you know, OK.

Participant_N   18:34
Yeah, 'cause we don't, we don't really leverage the access packages today. We've done a little bit of testing with it, but you know, we seem to go down that path and then it's like, oh, we run into that license requirement where we need either the the, the Entre suite or the identity governance license, you know?

Participant_D   18:38
Yeah.
Yeah, yeah. No, I hear you. Yeah. You mentioned guest accounts, right? Where are those guest accounts coming from and you know, how do you govern those and you know, what are the, what are they, what are they accessing these days?

Participant_N   19:08
Um.

Participant_D   19:10
Or have it to access.

Participant_N   19:11
Well, from from the Acme-Sub and ACME perspective, we have a cross tenant synchronization set up between the Acme-Sub and tenant and they just get provisioned as guest accounts within our tenant.

Participant_D   19:18
OK.

Participant_C   19:22
Is it B to BB to B or OK, OK.

Participant_D   19:23
Uh, OK then.

Participant_N   19:25
Yeah, yeah.

Participant_A   19:29
Oh, what are these accounts used for?

Participant_Q   19:30
And then we do, we do have have third party vendors as well that come in and do some development work for us, right?

Participant_N   19:33
Oh, yeah, yeah.

Participant_D   19:35
OK. Do do they could do? Yeah, do.

Participant_C   19:36
Yeah, that's that's that that that's very standard, right. I'm just we're just talking about these two tenants, right. So if there is synchronization happening and if there is AB2B connecting or connector that's used and then I don't know how you have connected the tenants, are you?

Participant_D   19:41
Yeah.

Participant_N   19:47
Yeah.

Participant_C   19:51
You know, doing a subscriber and publisher model, you know or or you just adding or inviting Acme-Sub to audio and vice versa.

Participant_N   20:01
We do have Lighthouse set up for some of the resource side of things, but as far as the Entre identity goes, it's just the the cross tenant sync configured.

Participant_C   20:08
Hm.
OK. OK. OK. That's good. Good. Perfect.

Participant_D   20:13
OK.
So, so when you so you're leveraging cross that in sync and from from the ACME to Acme-Sub and so you OK.

Participant_N   20:22
Other way around actually Acme-Sub into ACME so 'cause there are dynamics instances in in the ACME tenant so.

Participant_D   20:26
OK.
Oh, OK, OK. And then you mentioned contractors, right? Or or non ACME or non Acme-Sub resources, which which tenant would they be invited to and and their their account would be sourced from?
Would it be on the Acme-Sub side or the ACME?

Participant_N   20:48
I think it depends on which resources they need to access, but typically there'd be ACME.
I don't know. I have to defer to Participant_J. I don't know if he's set up any. I don't think he's on the call today, but I don't think he's set up many guest accounts on the Acme-Sub side.

Participant_D   20:55
OK.

Participant_Q   20:56
Yeah.

Participant_D   21:04
OK.

Participant_N   21:05
Um.
Yeah, I think we, I think for that that cross tenant sync though, I'm pretty sure that we trust like their MFA settings.

Participant_D   21:07
So like, um, OK, OK.

Participant_N   21:15
If that makes a difference or not.

Participant_D   21:15
Yeah that that yeah that that's one of the settings that you can you can configure right between the two like you can evaluate the device posture and or honor their MFA, right. Basically you know you know when you're using that that.

Participant_N   21:24
Mhm.

Participant_D   21:31
The guest account, it's basically leveraging federated authentication. So what happens is, you know, as a as an identity attempts to access the resource or the workspace, and if they don't have an auth token yet, they're redirected to their home tenant, right? Which then they receive that that token to be able to access that resource.

Participant_N   21:36
Sure.

Participant_D   21:51
But it it sounds like you know, I guess in the future how many users where I'm getting at is is the governance around the identities, right. So often there's you know you have cross sentence synchronization which facilitates the synchronization of identities between the two resources.
And then you know we would use identity governance to govern the entitlement management for those resources. You can also use identity governance to govern the OR manage the the B to B object invitations also, right?
Otherwise the you know what what happens is you know an administrator has to invite the user right and then next thing you know over a period of time now now you've got you got a a lot of of of guest accounts that you have to manage and you know the question is.

Participant_N   22:30
Sure, mhm.

Participant_C   22:40
Yes.
Yes.

Participant_D   22:40
You know, typically from a security perspective, like is there any access reviews to them? Do are those still folks still around? Because it it could open up a security vector of, you know, if those accounts aren't, you know, even needed anymore, right? It's it's a matter of revocation.

Participant_N   22:55
Yeah.

Participant_Q   22:56
We actually, yeah, we started that process and there we, Participant_N has has removed quite a few that we've just kind of evaluated, right, Participant_N?

Participant_N   22:57
Alright.
Yeah, I can say from experience that you know the the sys admins are usually the last people to know when a vendor is no longer engaged. So yeah, the.

Participant_D   23:13
Yeah.

Participant_Q   23:15
Right. No. And then and that's why we're gonna try to do some auditing. You know, what do we got, what do we need kind of a thing.

Participant_N   23:21
Yeah, we we set up an expiration date on the new ones going forward. But you know, we also have an automation script that runs that disables any of the, you know, stale accounts per se.

Participant_D   23:22
Yeah.
Yep, Yep. OK, good. Like what is your guys familiarization? This is more for maybe a future call like of, you know, Audra's identity governance and how this reference architecture works with B2B accounts, etcetera.
Is this something that you guys want to know more about? This could be something, you know, for a future conversation.

Participant_N   24:00
Well, I can say that we did evaluate like the getting identity governance licensed for all of our users in our tenant and that wasn't we didn't see, you know, feasible.

Participant_D   24:14
Mhm.

Participant_N   24:16
Workload for that I guess to justify the cost, but you know if there's some discounting available and you know this is a a definite use case that we can maybe get upper management to buy off on it, probably revisit it.

Participant_D   24:19
Yeah.
Yeah.
What about, yeah, yeah, no worries. What about in the ACME side? You know, do you do you happen to know if they're using any identity governance solution to manage, you know, users like, you know, a sale point or anything?
I'm just looking at all options that we can make recommendations for if if IGA is not available.
Or there's like a software that does that.

Participant_N   24:54
Yeah, I yeah, I don't think we're using anything today for that, I mean.

Participant_D   24:59
Uh.

Participant_N   25:01
You know, like Participant_Q alluded to, we're just using Pam and I think Participant_M was evaluating some on premise Pam solutions as well. But as far as lifecycle management goes, we don't really have a whole lot of that. It's just all homegrown scripts and whatnot, so.

Participant_D   25:09
OK.
OK, no worries.
And then when you when you write these homegrown scripts, what what language are you guys using?
Like PowerShell Csharp.

Participant_N   25:24
Yeah, I yeah, I use PowerShell. It's typical tip.

Participant_D   25:27
OK. The other reason I'm going out there is, yeah, typically, you know, like if we if we go to that level, you know there's certain certain use cases or or you know, functionality, operational functionality that you'd want to incorporate into your your script, right?

Participant_N   25:42
Sure, Yep.

Participant_D   25:43
You know that that's, you know, you know, in my experience, right? Like, um.
You know when when there is no tool available, we could do it programmatically, right? Like just running on scheduled jobs and then you know from you know when we're building out, you know the logic programmatically, you want to make sure there's appropriate logging so we can have the proper reporting to have the, you know, appropriate visibility to those identities to meet.

Participant_N   25:53
Mhm.

Participant_D   26:09
The, the, the, the governance that we're trying to achieve. The challenge you run into is, you know, it becomes all custom, you know, custom, custom code, which you know if the author leaves the organization, it becomes, you know, kind of, you know, sometimes it could become a black box, right?

Participant_N   26:12
Sure.
Sure, Yep.

Participant_D   26:26
Otherwise, you know leveraging your existing tool set if it's there, right? That might be a conversation with the RDL security folks. If they have an existing IGA tool set in place, can that be leveraged to, you know, to mitigate some of the security security gaps that might be in the solution?
Of course I'm talking long term and then the obviously the third option is you know looking at what CloudProviderA has out-of-the-box, but you know to meet that which you know we have reference architectures. But the challenge there is you know Participant_C, myself, Participant_A and and Participant_B, we're not licensing people right that that might come.
There there's a whole licensing type discussion that would happen that that might need to happen there, right?

Participant_N   27:10
Yeah, Yep.

Participant_D   27:12
OK. All right, perfect.
Sorry, Participant_A, go for it.

Participant_A   27:22
Yeah, I was curious when you were talking about audiences and and identities and.

Participant_D   27:23
Yeah.

Participant_A   27:33
And I know that we got Power BI, we got BI folks on the call. I want. I was curious about the the scope of the audiences using Power BI workspaces that you foresee today and in the future.

Participant_G   27:51
As far as how much, what you know, how much of the company are using consuming BI, I mean our.

Participant_A   27:56
Well, just I guess, I guess the the scope is like, OK, so you're tying things together. We got, you know, data in in fabric, we got RBI sourcing data from fabric.
You'll use our BI data deployment pipelines to deploy across environments like dev, test, production, right? Those will be separate workspaces. Those workspaces allow you to segregate who can do what.

Participant_G   28:24
Yeah.

Participant_A   28:32
So like a dev workspace that you're developing in and then you can deploy into a test workspace that a certain number of users are gonna have access to and then you deploy approved.
Reports, for example, and semantic models into into a production workspace and who are going to be the audiences in each of those environments and in the production workspace? Like how expanded is that? Is it going to be just internal? Is it going to be?
Ioan and ACME will be the will there be external consumers of that data, that kind of stuff.

Participant_G   29:08
Mm.
Sure.
Not so, not so many, many external, but but we're working out how to expand power users, the analyst community to, you know, do more work in the model space because.
We don't want to, we want to be able to empower more of the company across this across the fabric because just because of that there's already demand for you know to leverage copilot in Power BI and tools like that so that we can.
You know, build up those those skills across the org today. Today only developers access, for the most part, access workspaces with very few exceptions.
And really Power BI workspaces. We have been using those as really as data marts for development and then apps. The app publication is the release mechanism for Power BI to the end users.

Participant_A   30:09
Mhm.

Participant_G   30:23
Who are allowed in the app through security, that security layer. So if you have access to the app then you're then you're able to consume that.

Participant_A   30:37
OK. What's the spectrum of users? So you got like, you know, executives who want to see dashboards with KPIs you've got and then?

Participant_G   30:47
It's everybody. You're you're right. It's everybody from from every level of the org. I mean historically more than 5000 users here.

Participant_A   31:01
Are going to be looking at reports.

Participant_G   31:03
Yep.

Participant_A   31:04
What's first?

Participant_G   31:04
Of course you've got different. I mean, not everybody has access to everything. The highest level of the company, V PS and above have access to everything. They can go peruse apps and find what they want. Of course they pin their favorites and go from there, but.
Depending on what their role is, but but otherwise apps are you know.
Parsed by team and and function.

Participant_A   31:34
OK, OK. And what about power users, people who just want to like sift through the data themselves?

Participant_G   31:40
What's that? There's a pretty, pretty small bunch really. There there are, there are a few and we want to begin to do more with those folks because yeah, they are asking for more.

Participant_D   31:41
Uh.

Participant_A   31:44
OK.
OK, OK.

Participant_G   31:58
But yeah, the part of the part of the the biggest power users will have an app they use and they download the data and play with it and and you know, working with it in Excel and really dive into the details.

Participant_A   31:59
Because.

Participant_G   32:16
We've done, we had a couple folks want to play with Power BI copilot and and we've enabled a bit of that. Go ahead, Participant_D.

Participant_A   32:25
OK.

Participant_D   32:26
Oh, good. Good for start, Participant_G, you know.
Are you done or did you finish your thought? I wanna, I just don't want to interrupt. Yeah. So quick question. You mentioned you like you want folks to be able to download the data, right, and play with it. You know at other customers that you know, depending on where they're coming from, that could be restrictive, right?

Participant_G   32:34
Oh, yeah, yeah, that's.
Yep.

Participant_D   32:49
Because you know in your guys use case, right you are are you gonna allow that you know are the users accessing the you know the various fabric resources are they gonna be coming in for you know.
You know, through managed desktops, you know, unmanaged desktops, etcetera, right? Um.

Participant_A   33:06
Yes.

Participant_G   33:13
Yeah, they're largely moving to, you know, managed desktop, but at the same time, there's not.
There's not an issue with with folks, you know, doing a drill to detail and then downloading that data. I mean, we haven't had an issue with that as far as.

Participant_D   33:31
Yeah, I guess is, yeah, is the data, you know, is there proprietary information in that data that may not want to be, you know, you want to, you want entirely control, right.

Participant_G   33:41
Well, if they've got access to the app, now if they've got access to the to the app itself, they they belong to the data or have right permissions to the data.

Participant_D   33:54
OK.
Um.

Participant_A   33:57
Does that data what kind of of of of power user analysis is being done? Is it going to be like summarization work on summarized OT data for example, or is it largely financial?

Participant_G   34:14
It's a bit of everything really from and even probably mostly it's, it's managerial and operational data that we're looking at in in today. I mean we've got a few financial apps, but that's not the bulk for sure.

Participant_A   34:32
What is? What is the bulk? What's what's your your core audience for like?

Participant_G   34:35
Think think equipment and.
You know, product and parts and services and so all of that agronomy for growing for the farms and you know, everything to do with it.

Participant_A   34:56
OK.

Participant_G   34:57
Mm-hmm.

Participant_A   35:03
So the the that that reporting space is included in like the scope of work that we're talking about for Acme-Sub and.

Participant_G   35:13
Acme-Sub and they're in the same boat, right? They've got a little bit of everything going on from financials to, you know, operational to managerial reporting.

Participant_A   35:24
OK.

Participant_G   35:25
And they can chime in. I mean, absolutely.

Participant_E   35:28
Oh, that's absolutely correct.

Participant_G   35:29
Yeah.

Participant_A   35:32
And then do you foresee this as being more reporting as being more of an Acme-Sub function or more of an ACME function?

Participant_G   35:40
It's both.
But we don't have folks that cross the streams too too much. I mean, folks who are consuming Acme-Sub reports are from Acme-Sub and.

Participant_A   35:59
So do you think it would make more sense for reporting to sort of live in a an ACME tenant with access granted to?
to Acme-Sub and folks for Acme-Sub and related reporting.

Participant_G   36:16
That's more of a question for you guys, I think. What makes more sense? That's the thing. That's a question that we've been having for a while. I think across the team is, is what's the best way to manage the cross tenancy?

Participant_A   36:19
OK.
I mean it's.
OK.
Yeah, I mean, I guess the reason I asked the question is that's what it sounds like to me, but I'm I'm listening for a reason why that would not be the case.

Participant_G   36:37
H.
Hmm.

Participant_A   36:48
And what's the counter argument?

Participant_D   36:56
What was the What was the Doc Participant_A?

Participant_A   36:57
Is.
Yeah, the thought is, OK, So what I'm what I'm hearing sounds like a a a a strong argument for basically saying, look, you've got ACME owns the the reporting tenant owns the the reporting.
And Acme-Sub and has access to that reporting for Acme-Sub and specific data and ACME you know folks can can view ACME specific data and in the future other subsidiaries can also look at you know workspaces.
You know, reporting workspaces in that ACME tenant that are specific to that subsidiary for example, that sounds like the the sort of natural setup that you're describing. What I'm hoping to hear is it sounds like you.
What I heard you say is you've been having this debate internally.
Inherent in any debate are two sides.

Participant_Q   38:00
OK. So to my perspective is that we, you know when when we started this journey with them that we we put it in our tenant just because you know even like with EntERPSys_A is our you know that kind of thing.
And it just kind of natural evolution of of that in terms of we'll just put it here because this is where am I right Participant_E with that?

Participant_E   38:33
I'm I'm not sure why EntERPSys_A went into RDL's tenant to be honest.

Participant_Q   38:37
Right. We were thinking about breaking them apart, but for some whatever reason and I wasn't, you know, I don't think anybody on the call was part of that decision process, but it just seemed like more of a natural thing. But but we we were always talking about combining tenants and we just haven't done it.
And so that's where you know the the questions of of while we we do it this way because this is the way we've always done it kind of a thing. And so we're open to to suggestions in that regard as long as it's not too painful for everyone involved to.
Make changes.

Participant_G   39:13
Yeah, I completely agree, Participant_Q. I'm not sure who had determined the tenancy design and and if if there's anyone involved at CloudProviderA or or how that came about.

Participant_D   39:14
Mhm.

Participant_N   39:26
Yeah, I was. I was involved in those initial conversations. I don't know why the decision was made that way, but they think that was an initiative from Brad and leadership. They wanted to because you know, ACME is gonna be leveraging dynamics in the future and I think that was a good use case, a business, first business use case to have.

Participant_G   39:32
Mhm.

Participant_N   39:44
A Dynamics environment, you know, in the parent company's tenant.

Participant_G   39:49
Mhm.

Participant_N   39:49
I don't know if it was a for a licensing break or what it was the the actual final determination of that. But yeah, we like like Participant_Q alluded to, we were talking like a full tenant migration into ours along with full on Prem domain migration.

Participant_G   39:56
Yeah, no idea.

Participant_N   40:05
They had a legacy ERP system that did not allow a smooth transition for the on premise stuff. So then the the migration was tabled at that time.

Participant_D   40:12
Excellent.
What does? Um, sorry, go ahead.

Participant_Q   40:19
You let it happen, Participant_F.

Participant_G   40:19
OK.

Participant_Q   40:22
I just said, so you let it out.

Participant_D   40:26
What? What? Like, OK, so everyone on the call, they're primarily from Acme-Sub, right? Like, I don't. What does the Acme-Sub infrastructure look like? Like is it, you know, we got, I know we got some tenants, we got office, we got fabric, but is.
Does your guys identity reside in its own like completely separate Active Directory on premise?

Participant_E   40:53
Yes.

Participant_N   40:53
Yep.

Participant_D   40:54
OK.
And how big are you guys, Acme-Sub?

Participant_A   41:00
Mm.

Participant_E   41:03
maybe a couple of thousand. I I wouldn't know. I I need to go and look how many people are in the D.
I assume or maybe I'm answering wrong question. What do you mean by how big? Like what we span or how many people or?

Participant_D   41:11
No, no worries.
Yeah, yeah.
Yeah, yeah. I was just trying to understand some of the dependencies that would, you know, discourage a, you know, consolidation effort, right? Why did they build out? Why? Why did you guys build out separate infrastructures, right?
Um, you know what? Why? Why is it? Why is it that way, right?

Participant_E   41:37
Yeah.

Participant_N   41:39
Oh, that.

Participant_E   41:41
Yeah. So as an example, like Power BI, we have our own tenant that's tied to our AD, right? So all of our Power BI users are our tenant users. They're not ACME tenant users necessarily, so.

Participant_D   41:47
Mhm.
Mhm.

Participant_E   42:00
Or all of the licenses for those users I tie are tied to our tenant as well.
So for us, for example, for me to go into ACME tenants and consume something out of that, I believe I I do have to get another license assigned to my account, whether that be guest account or another ACME account or whatever that.

Participant_D   42:27
Yeah, it depends.

Participant_E   42:28
May look like and Participant_J just joined. He may be able to speak quite a bit more about how our AD is related to to Rd. or AD and what what are the potential issues when thinking about combining tenants.

Participant_N   42:48
Yeah, a little, a little history of that. We are operationally separate for prior to this this project, so prior to Dynamics. So that's that would explain why they have two, two separate tenants.

Participant_D   42:49
Hey, Participant_J.
OK.
Can you guys see my screen?

Participant_N   43:05
Yep.

Participant_J   43:06
Yes.

Participant_E   43:06
Yep.

Participant_D   43:06
So this is a sanitized document, kind of what you know, kind of what I used to provide that visual. As I mentioned, there's, you know, Participant_A and I were looking at the various artifacts earlier last Friday and you know, this is the stuff we'll start getting into.
Talking about related to you know, you know what what what exists today, right. But you know ideally what we'd be looking at is going to this level of detail, right, 'cause this from a security perspective, this allows me to kind of.
You know, look at you know each resource, ask specific questions of how you know how access is governed, you know the type of data etcetera and the layout, right. So you know you guys would have a visual of you know what's being built out, right. Typically we'd have you know kind of a an subscription, right. I think you guys had.
Shared this too and we just got to update this. You have your various ADLS resources, how this is structured. You know these are some of the the capabilities within which you know there there would be more talking points or you know drive recommendations on how you would govern these various resources.
And then you can see, you know within subscription you have your capacity, right? Which then you know we start going into the fabric security architecture itself, right? You know how you know identities are are governed via authorization into each workspace. Here's the various types of data.
And then you know within CIC processes, you know, you know, you know, ideally you're using Defender for Cloud, which is a posture management. I think Participant_J mentioned you guys are using Policy. So that's a good thing. What what, you know, Policy essentially is.
It's a configuration baseline or you know that you know that allows them to govern what kind of resources and what kind of configuration that gets deployed, you know, versus, you know, just allowing allowing developers to kind of deploy resources ad hoc, right?
You know, you can specify specific configurations that they're locked down. Specific configurations we'd be looking at is, you know, authentication, authorization of this resource, right? Firewall settings, data encryption, monitoring and logging and then and governance, right. So you know these, these are some of the, you know as we go deeper.
Into like I know you guys have mentioned that the CICD processes is still fairly, you know, early or green, right? These are specific talking points that you want to make sure that are that are going to be incorporated if you're building out resources because what would happen is.
If they don't comply by a specific configuration, the policy might just, you know, restrict the the the resource from being created and you know during that build process it would fail. You know, like I said, what we're looking at is.
You know where the resources are being configured, how the you know you know what is like you know the data security controls are placed and then you know the the authentication or identity services onto itself.
Um.
This diagram here kind of shows how users are accessing the various fabric resources, right? Do you have option one or option two? You know, I think you guys are leveraging option 2, which you know, you know we'll go into more conversations on like do you guys use a conditional access policies and then what are the specific criterias?
That you're evaluating to determine access into the fabric tenant, right? That could be based on users, groups, applications, network locations, devices. Now this is probably going to be applicable because you know what we'll do is we'll have to tie.
This could be from what I've heard. It could be a guest or a regular user account that could be sourced from, you know, cross head and synchronization, right?
You know, this kind of goes into like the outbound security of how various research within a workspace is being accessed. But this one is kind of the visual of, you know, how we we architect access or manage access into the workspace itself, right?
We, you know, we leverage, you know, conditional access policies which basically enables us to evaluate or interrogate the authentication request or the access request to resources, you know, during the authentication request itself. So meaning that.
You know when a when a user attempts access a resource, the service is going to look for an auth token. If one doesn't exist, it's going to redirect the user to its identity provider or trusted issuer to obtain that authentication token. What happens there is you know the request will go to Entre ID.
It's going to challenge the user right for a specific credential and then we process the conditional access policy which we evaluate those specific criterias to determine whether the users, you know, should have access or not, right. It's really a zero trust architecture model that we're that we would.
That we'll go into a little more as we learn more about your environment and your use cases. You know, obviously from a security perspective, we want to make sure that the appropriate logging is is being captured and security has visibility. So for example within you know the Entre tenant, you know the.
At least the Acme-Sub one that you know the sign in logs and the activity logs are are being forwarded out to a Log Analytics workspace that could be ingested through your SIM, right? And then there's specific alerts that we can share with you. You know with the security team on these are the specific alerts that you should be keying off of.
Right. So I I do plan to give that to you guys as we learn more, right? Um.

Participant_M   49:07
Yeah, and I'll just chime in here too and add that we've got all that set up activity log sign insurance going to SIM monitored by an MDR provider. So I'm not anticipating we'll have to spend too much time on that. We're already doing device.

Participant_D   49:21
Thanks.

Participant_M   49:26
Appliance checks, MFA, all that good stuff through through conditional access, but I think it would be interesting. I mean if there's more of a tie in into fabric, that's a newer thing for us. So it'd be interesting, but at least.

Participant_D   49:27
Perfect.
OK.

Participant_M   49:43
I feel pretty good about the the conditional access setup at least.

Participant_D   49:46
Perfect. Yeah, yeah, definitely. Right. You know, before you came in, Participant_M, you know, I think you know some of the things we were talking around with like what we were talking about was just, you know, role based access control and the governance around the types of users that access this, right.

Participant_M   49:49
Oh.
Mm-hmm.

Participant_D   50:04
You know this diagram just talks about the data security. It goes into a little more detail within the workspace roles itself. Like you know if this is sufficient and then we can go down. You know if you guys do have requirements related to item level security like role based or.
World level security, yeah. And as I mentioned a lot of this through our discussions will probably you know probably an output from the from the from the deliverables will be you know just a summary with recommendations on.
On you know where you guys are at today, here's some, you know, gaps or recommendations to improve the security posture over the the overall solution itself, right. So you know, I'm kind of showing you under the kimono what's being planned, you know, for future discussions.
Um.
So I think it'll be good. You know, kind of pivoting to this conversation right here. This is really the there were two diagrams that we wanted to talk to towards. One was, you know, this architecture here just to understand how the data is flowing all the way up to the various environments.
We may have some questions there and then there was another diagram which was the future state, which was the the future states.
The future state IT architecture and you know, you know, we want to actually talk about this, like how did this come out or who, who, who drew this was kind of me and Participant_A's question, right?

Participant_A   51:44
Yeah.

Participant_B   51:44
Yeah, this was labeled Future Future, I believe.

Participant_D   51:47
Yes. Yeah. Future, future, right. That's how we caught it. And how does it relate to that previous diagram that we were looking at this one?
So with that, Participant_A, do we want to start diving into this, into this, you know?

Participant_A   52:07
Or do we want to take a like a 5 minute break because we've been at this for about an hour. I want to be mindful that people.

Participant_D   52:14
OK, either way, let's let me check back, let me check back.

Participant_A   52:15
Might need to get some water, some coffee.

Participant_B   52:16
Yeah, let's take a let's take a break and and re re join at 5 minutes after the hour. Make it 10 minutes, OK.

Participant_D   52:23
All right, I'm gonna get a cup of coffee. All right, I'll be back. Thanks, guys.

Participant_G   52:24
Sounds good, Kip.

Participant_A   52:24
Alright.

Participant_G   52:26
I.

Participant_B   1:01:16
OK, why don't we look to get started again?

Participant_A   1:01:23
Sure.
Give me one second here.
So.
All right.

Participant_C   1:02:08
Let's check if everyone is back, Participant_A.

Participant_A   1:02:10
Yeah, let's get a get a check.
Can we get a a reaction? Who's who's back?

Participant_R   1:02:28
OK.

Participant_A   1:02:28
All right.
That looks good. OK.
Participant_D, I think we have a we have enough people.

Participant_D   1:02:42
Alright.
Right. So last last Friday, Participant_A and I were reviewing the notes. You know, we kind of went through the various shared artifacts, you know, and this is kind of where we landed. You know, we did see, you know, this, this to the left of the diagram.
And how it aligns to the ISA Dash 95 model.
That's we're assuming that's that's correct. That was the intention of the left hand diagram.

Participant_E   1:03:15
Yes, that was OT side, but yes.

Participant_D   1:03:18
OK.

Participant_A   1:03:23
Right. So you're you're covering all five or six layers of the of that the ISO 90 ISO 95.
Like because they have, if I just go and you know research ISO 95, they got like whole different management layers all the way up to the financials, right. So it looks like that's what you guys have covered in your zero through 5, is that right?

Participant_E   1:03:52
Correct. It's kind of both infrastructure and corresponding data.

Participant_A   1:04:00
intelligent manufacturing. Got it. Okay.
Actually, not ISO, OK.
So we think we wanted to go into the model side of this and the model the ML OPS. So maybe you could you could walk through your vision for and you know this diagram kind of like covers part you know much of this but what's your what's.
Your vision for.
Or the ML OPS side of this or aspect of this of this equation, right? How does it fit into this picture and what is that unified namespace accomplish for you in that?

Participant_P   1:04:50
So the unified namespace is a way to gather all of the data from all of different sources and make it available in the MQTT broker so that tools like Ignition or.
Hive MQ pumps data to the cloud and to the historian on Prem. So it's basically a big data model for all of the data that is necessary for the OT world to operate in a nutshell.
Um. And then?

Participant_A   1:05:27
In that in that in that unified namespace, are you are you saying that the the MQTT broker basically has a multiple multiple subscriber model?

Participant_P   1:05:43
Yep, Yep. There's, you know, clients that subscribe and publish messages to the MQTT broker.

Participant_A   1:05:51
Right. So you're you're publishing basically in addition to you'd be publishing. In addition to going to the cloud, you'd have one going to the historian. In this case the I I need to pull up the diagram here.
Give me a second here the.
Which one is this? The Ignition? The Ignition Historian?

Participant_P   1:06:24
Correct.

Participant_A   1:06:29
OK.
Please continue.

Participant_P   1:06:36
So yeah, that's that's what the UNS is gonna be built for. So that you know any new clients that may need data from it or publish data to it.

Participant_A   1:06:42
OK.

Participant_P   1:06:50
Can do so in a very efficient way and.
And yeah, it's just a central repository for data essentially. And then as far as the MLML OPS goes, what we're thinking is either.
Well, using the cloud AKA Fabric to train all the models and then containerize them and deploy them to the OT.
Side of things as a container and so from there there's multiple architectures we can go with for the container. So this container can host an API or it can also.

Participant_A   1:07:30
Mhm.

Participant_P   1:07:43
Connect to the MQTT broker, receive the inputs. It needs to calculate the outputs and then publish those outputs back to the MQTT broker and then from there it's available to all of the different clients that are subscribing to the broker.

Participant_A   1:08:04
OK. And what's your, what's your vision for the public, the your model training, the source control around that and?
Basically publishing a a trained model out to your on Prem to your edge.

Participant_P   1:08:27
One of the options we're looking into is ML flow and with the help of experiments and runs and all that stuff, kind of help us track what's been, you know.
What trials have been done as far as the ML training goes and then once a final model has been developed, maybe publish that one to get so that we keep track of kind of like the models that run in production.
And then from there containerize the model and deploy it into.
On premises.
It's kind of the the flow that we have in mind. We're definitely open to, you know, better solutions.

Participant_A   1:09:09
OK.
Yeah, I was curious like your public, your publishing method for getting it to Edge.

Participant_P   1:09:25
Um, what was it called, Andrea? Was it?
That one in mind.
It's in like one of the diagrams you showed earlier, Participant_A. The future future state.

Participant_A   1:09:43
Oh yeah, Participant_D had the future future state.

Participant_P   1:09:49
But yeah, I think we're looking into.

Participant_A   1:09:53
Well, so I see ACME Arc here, which is your control plane, right? And then get repo docker registry. So there's a there's a model registry, right?
So you register it and then you can release it from the from that registry.

Participant_P   1:10:17
Yep.

Participant_A   1:10:20
But the actual mechanism of getting it to into a space. So I know for example that IoT Edge allows you to effectively automatically push it to Edge where it gets automatically hosted.
I mean, that's a that's a mechanism there. That's why I was like curious if you have a a counterpart.

Participant_E   1:10:42
OK.

Participant_P   1:10:46
No, not that I know of so.

Participant_E   1:10:50
Are are you are you talking about the difference between like pulling the model and pushing the model or what are you?

Participant_A   1:10:51
OK.
Yeah, so like imagine you. Yeah, imagine that you're you're training a model. OK, you've you've if it's now in a registry for example, and you're using ML flow, but how do you actually get it to your on premise environment to get it?
Posted there.
And like automatically just get it pushed down there and hosted. Think of it like a deployment pipeline.

Participant_E   1:11:27
Yeah.
Yeah, yeah, like in. OK, Full disclaimer, this future future state was built like year and a half ago, so a lot of things changed. So you can judge it conceptually, right, but not to the point of using specifics.
And our thoughts, our initial thought was it's gonna be just, let's say Kubernetes containerized deployment on Prem and we'll just hook it up to JHCR from there to repository and be able to pull.
A model and do a rolling update of the model and just run it as a dockerized image from there. There are also a lot of different tools available that like Flux or there's a whole bunch of things. I don't remember what they're called, but.

Participant_A   1:12:18
Mhm.

Participant_E   1:12:27
Is one of them that is capable of checking in on repository and whenever a new version is published, it can automatically pull and use YAML instructions to deploy that model as well. So those were just some of the thoughts we're thinking.
And obviously we hope that this kind of can firm some of our thoughts or point out some of the things we may be missing around it. And this diagram is sort of like, oh, if we were to put together the data movement of what we think it.

Participant_A   1:12:46
Mhm.

Participant_E   1:13:06
Will be. This is what it is, but it's not tied to like specifics, let's say.

Participant_A   1:13:13
Got it. Yeah. So like you said, it's a year and a half old, so it's probably a little bit dated in more than one way as in your thinking has evolved and also the technology landscape has evolved in the last year and a half, so.

Participant_E   1:13:26
Mhm.

Participant_A   1:13:27
I'm I'm hearing, among other things, that you're how to put this.
You guys look at a lot of um.
Specialized pieces like you mentioned Flux and other tools that you're trying to kind of use a bunch of different pieces of tools and technologies to kind of cobble together an end to end solution.
Does that sound right?

Participant_E   1:13:57
Yes and no. It it's it's a choice of choosing the best tool in every specific application and then if that doesn't trump a unified platform.

Participant_A   1:14:01
OK.

Participant_E   1:14:13
So it's it's a trade-off, right? If we looked at, for example, IoT operations a year and a half ago and it couldn't do anything and it was a lot worse than anything else that we looked at. Obviously we had to decide to stay away from that, but as time goes on and things mature and like the.
Deployment methods mature if there is like a unified deployment of the from pipeline to the GHCR and then pull into the on Prem to containerized deployment. Obviously if there is a tool like that and it's very good.

Participant_A   1:14:46
Mhm.

Participant_E   1:14:52
That's we're not married to using like GHCR instead of the Docker repository for example, or using a specific method of deployment like Flux, you know, or pushing versus pulling the model.

Participant_A   1:15:00
Right.
Right. Yeah, because it sounded like with flux, you're basically doing a polling method to like, is there something new? Is there something new? And then when something new and then it then it takes action, but it's polling, you know, on some kind of intermittent schedule.

Participant_E   1:15:21
Mhm.

Participant_A   1:15:27
Um.
I think that regardless of the.
Whether you know which which kind of tool sets you you choose to use, you will probably end up hosting containers in Kubernetes on premise one way or another.
So I wouldn't change that part of the the diagram. I think that's going to remain fairly that's a consistent.
Way of doing it. And the the reason I say that is let's say you chose to go with.

Participant_E   1:16:03
Mhm.

Participant_A   1:16:12
I guess what I could call um.
Even if you did that and you managed all of that through a control, through an control plane, and you wanted to host your models that way, it's still under the hood going to be using Kubernetes.
Kubernetes service under the hood.
Right, but um.
Depending on your your how to put this depending on your expertise with Kubernetes and your willingness to spend time managing all the intricacies and complexities of of Kubernetes with on on premise essentially comes you have the.
Option to use Container apps which are a managed platform as a solution way of using Kubernetes service.
You don't get all the bells and whistles per se, but you reduce the complexity. It is a way to simplify if that is if you don't need all the complexity and it.
Probably seems like not a bad idea because I don't think that for this particular case you need a lot of complexity, which means you can spend less time and energy and resources actually working on, you know, setting up Kubernetes and managing it and just say.
Look, I'm going to treat it like a platform as a service for example, and just host these things for me and you know the platform will take care of it for you. So like you said, there's some pros and cons if you, but if you need more control because you're under the hood, you still have AKS running, you could.
Fall back to that if you need to, but following, I like to follow the sort of agile development practice of don't build things you don't need to build, right? That's energy and cost and resources that you know it takes time to do this things that slows you down if you're building things you don't need to build.
I don't know that that's the case, but I'm just saying that that's a it's a consideration, so we say.

Participant_E   1:18:36
Yeah, no, absolutely. I appreciate it, Participant_A. And I know you were looking into IT Edge as well. I know that we haven't looked into that and I think it has the option of deployments of the models or used for that. So absolutely.
Like either Helen specific side, we are smaller team, so we prefer not to create a lot of overheads in what we do because then you just end up managing things instead of actually making.
Difference. So we we're absolutely love the idea of the workshopping about this and kind of chatting back and forth and answering some of your questions and hearing some of your concerns.

Participant_A   1:19:25
Yeah, I mean we'll we'll learn more I think and and it's it's there's nothing I wouldn't say we're going to be, we're not going to be overly prescriptive. This is it's always going to be an exchange.
You know, we'll probably propose something here or we will be proposing some kind of like architecture and then we'll workshop that with you and like hash out, well, this is.
Actually, there's a problem with that or there's a problem with this and we can, you know, so it'll be iterative, absolutely.
Yeah, I guess I'm just confirming or validating, you know, some of the things you're thinking right now.

Participant_E   1:20:08
Mm-hmm. Absolutely.

Participant_A   1:20:12
OK, so we've got um.
Yeah, it sounds, it sounds pretty solid. Great. What I'm hearing so far is just ML OPS, ML flow. You'll be using GitHub Enterprise to manage the, you know, the source control side of this.
And it'll flow and then you know there are some there are some point things we'll we'll try to workout whether it's.
Whether it's IoT Edge or it's some other mechanism, but ultimately you're gonna have Kubernetes on Prem.
And all this is going to be run inside the Acme-Sub and tenant, right?

Participant_E   1:20:59
Yes.

Participant_A   1:21:01
Yeah, OK, that part of the game map for me feels.
Mike, I understand that. Can we go back to the other, the other one?
The other, uh, yeah, that one.
Now I apologize, I is it possible to make that a little bigger?

Participant_D   1:21:30
Woodwitch area. Um.
Which which area do you wanna look at?

Participant_A   1:21:36
There, yeah, let's let's for now, let's go to the the the stack on the left.
OK, so you've got ignition, you've got PLCS. What part of this layer is is talking to the broker or is it just high byte that's doing all the translation and communicating with the broker?

Participant_P   1:22:09
Yep, that's a high bite is kind of like the the bridge you could say between ignition and the broker right now.

Participant_A   1:22:19
OK. Are there other things that are in that stack that need to communicate besides ignition?

Participant_P   1:22:30
To the broker.

Participant_A   1:22:34
Well, I'm looking for, I'm just trying to fill out the game map as it were. So I guess part of that is.
I see a lot of little blocks and they're doing different things and I'm not sure what all of them are doing and if there's a part of this that we need to understand.

Participant_E   1:22:53
I think an easy way to simplify understanding of the broker UNS is just kind of a pub sub hub of all the information at once so.

Participant_A   1:23:05
Mhm.

Participant_E   1:23:09
As an example, everything, all PLC data, all SCADA, all every layer at the plant kind of gonna push the data into a data model in UNS broker.
And then clients, for example, we're looking at CMMS right now that will consume some of that data and return some of that data back. So the the idea behind the whole thing is that everything.
On the plan floor is going to be talking into one spot and consuming out of one spot. Instead of talking layers from layer zero all the way up to layer one to layer two to layer three, you have that one source of truth on the left and every layer can talk there.
Aaron back.

Participant_A   1:24:04
OK, so you're saying layer 0 is talking to the broker. The broker is talking back to layer one, for example.

Participant_E   1:24:13
Yeah, there is capability for that. So historically I think we are more like old school operations where layer 0 can only talk up to layer one, layer one can only talk up to layer 2 and so on and so forth, right? How data moves.
In the OT side and this is kind of a little changing that from the data access perspective. So there is still communication from layer 0 to layer one to layer two to layer 3, but it's not business well, it is like business necessary.

Participant_A   1:24:40
Uh huh.

Participant_E   1:24:52
But you also have UNS that has the entirety of the data. So for example like PLC's specific motor is running at a specific speed, right? That is gonna be in the UNS and then when we set up a.
CMMS system, it will read how long was this motor running for this specific speed and then tell us return back into UNS, hey, this motor is gonna break into like 3 months based on predictive maintenance as an example. So that UNS layer is kind of a layer of.
Interchangeable information between, if you will, not necessarily boltons, but like integration hub or some sort.

Participant_A   1:25:44
So one of the things I'm hearing there is you are sending telemetry data to the broker to transfer up a stack, but potentially elsewhere as well.

Participant_E   1:25:54
Yeah, exactly. It's like a access point to OT data for anything that that speaks Pubsub.

Participant_A   1:26:03
This is.
So this is a good example to just kind of like break into this part of the conversation. I think where else would that telemetry data do you envision going?

Participant_E   1:26:16
Anything that we install for OT use, so like CMMS is a good example or any kind of reporting systems or anything that's gonna be on Prem.
Belonging to the plant that can't speak pub sub is gonna give it its data and consume data out of it.

Participant_A   1:26:43
Do you imagine also logging or recording that in the cloud for your own analysis, or would that be covered already by a different system?

Participant_E   1:26:56
No. So UNS will in some sort be copied to the cloud. I don't know how much of it. We don't want just go nuts and copy the entire thing into the cloud, but the cloud's data lake will be UNS, but historically.

Participant_P   1:26:57
So.
Plus.

Participant_E   1:27:14
If you think UNS is just like what is right now, what is each motor telling me right now? What is each system telling me right now? And then we take all of that and pump it to the cloud and then use that in the cloud for future processing and analytics and other pieces.
ML.

Participant_A   1:27:36
Right. I'm 'cause I can totally imagine that, like, if you're a Formula One team, you're trying to tune that engine and trying to figure out at what point does this part break, you know, stuff like that.

Participant_E   1:27:47
Mhm.
Yeah, definitely. Like for example, for this specific ML project that we're doing right now, we only need maybe 50 data points out of 2000. So right now Participant_P is only consuming 50 roughly or 100 whatever data points into the cloud.

Participant_A   1:28:01
Mhm.

Participant_E   1:28:10
But let's say in eight months we have another ML project that will be adjacent to this project. We'll add another hundreds or if that makes sense.

Participant_A   1:28:10
Mhm.
Right. But you mentioned, for example, that you're trying to build historical data that you don't yet have, is that right?

Participant_E   1:28:32
Uh, yes, in the clouds. I guess that is accurate.

Participant_A   1:28:38
So.
Would it be accurate to say that it might be a good idea to try and build that history now in anticipation of oh, we now eight months later we have the bandwidth to do to work on another ML model and look, we've got eight months of data now.
to work against.

Participant_E   1:29:01
Yeah, yeah. No, absolutely. You you have a good point. I'm just thinking, I'm just trying to portray that we don't need to consume all the data at once, but it kind of makes sense, right? If we want to build a mail model in eight months, we can kind of open that faucet a little.

Participant_A   1:29:04
OK.
Right.

Participant_E   1:29:21
More and move more data into Data Lake. Or if we don't need some data, we can always close that faucet and only move the data that's necessary for specific applications.

Participant_A   1:29:26
Yeah.

Participant_E   1:29:37
So that that's the idea. Mm-hmm. Sorry.

Participant_A   1:29:37
Yeah, you can also.
You can also trim like the kinds of things that we're gonna be looking to do. Don't need the entire data set coming out of that that feed. We can trim it down to a much narrower data set, which you know, lowers the the data footprint and the storage costs and.
All of those things, right? You can certainly do that as well. I'm a little bit sensitive to this particular topic because I have lived through the experience of I need to do some BI reporting and on something and we don't have the data and we have to start building it.

Participant_E   1:29:58
Yep.

Participant_A   1:30:17
How do we report on this when we don't have the data yet? And the only way to get it is to just capture and start recording now, you know?

Participant_P   1:30:27
Yeah, no, it it it makes total sense. I think our current approach is pushing everything from the UNS to the data lake. Once we get to the delta tables, we only consume X amount of sensors.
And and we don't, we don't want to pull everything, but we do want to store everything in the data lake in case in a year or so from now if if a new use case like you guys are talking about arrives and we need different sensors for that project then we we.
We have that available, we just don't use it in our delta tables up to that point, if that makes sense.

Participant_A   1:31:11
Right, right.
Oh wait, wait, are you saying, sorry, I'm not sure I fully understand. Are you saying that you're gonna, you're gonna drop it and basically in your staging layer regardless, you just may not be doing it. You'll have the raw data, you just may not be processing it beyond there.

Participant_P   1:31:32
Yeah, yeah, you could set a filter from your data lake to your delta table to only consume X amount of sensors, and that way you limit the amount of data you're.
Using in your in fabric basically, but you still have that historical data in your data lake.

Participant_A   1:31:54
Mhm.
Right, right, right. As long as you have the source, you can always transform it later. You know, there's some work you have to do to, you know, clean data and prepare it and everything and that takes some bandwidth on the on the development resource side.
And so if you have the data, you can always, you know, pick it up later and say, OK, now we have a project where this is important to the business and we can start working on that. But look, we've got the source data, the original data already. So you know, we can start processing it now.
We're not starting from scratch. So if you're doing that, that's, you know you're and you're OK with the the storage cost on that, then you're good.
I think.

Participant_E   1:32:41
Yep.

Participant_A   1:32:43
OK.

Participant_E   1:32:44
That.

Participant_A   1:32:45
OK, um, let's see what else?
So you're going up through layer three and then layer 4 is basically already your Dynamics EntERPSys_A.
That's where you're like in the ACME Senate.

Participant_E   1:33:05
Yeah, and this specific diagram was built by OT, so it may not be as.
Accurate.

Participant_A   1:33:17
Say again, it was built by.

Participant_E   1:33:19
OT our our OT guys, yeah.

Participant_A   1:33:21
OT.
OK.
So you got SQL.
One lake, OK, M4 firewall. That's that's ACME's firewall, right?
Think I remember from from previous conversation already it was taking care of all the firewall.

Participant_E   1:33:48
Yeah, that was the thought.
I don't know if Participant_F's here, but the thought was, uh, the data is hitting ACME firewall.

Participant_A   1:33:52
OK.
OK, uh, so you got fabric Power BI.
Wait, I see.
B365 is exed out as well as.
I can't read what can access.

Participant_D   1:34:25
Can access, can access, can access.

Participant_E   1:34:25
Yeah, so, so they they thought EntERPSys_A was in our tenant, so they X'd it out and moved it to our DO tenant in the yellow.

Participant_A   1:34:37
Thank you.

Participant_E   1:34:38
Where if you see it.

Participant_A   1:34:39
Oh, I see. I see. I see. And this.

Participant_E   1:34:40
And and the VendorK is a solution that we're buying. It's it's not hosted by us.
As an example, it's it's on the right in the SAS group.

Participant_A   1:34:50
OK, so.
Got it. So you have to import data into Canaccess.

Participant_E   1:34:58
Right. Yeah, it potentially, it will potentially be feeding off of either data lake for some things or UNS, it's our planning software. So like production planning and sales planning, forecasting.

Participant_A   1:35:18
OK.

Participant_E   1:35:19
I

Participant_A   1:35:21
Participant_D, do you have any thoughts about the like this external software as a service and like access to data, how that works? Do you want any any questions around that?

Participant_D   1:35:35
Uh, which one? Which? Which part of the diagram? Um.

Participant_A   1:35:38
So over in the right, it says software servers or SAS. They got GEP. I'm not sure what that stands for and VendorK, which is like this some kind of planning software. So they're paying for that separately and they need to pull data into it out of.
And I didn't quite catch where which part of the the stack they want to pull data out of to get it into kinaxis if it's like.

Participant_E   1:36:03
Right now, yeah, right now we only have integration using Logic apps into EntERPSys_A. So it's like SFTP hits Logic app hits EntERPSys_A NTT, but in the IT has a lot of capabilities as a software itself.
Planning capabilities that we're not utilizing now. So potentially in the future we could utilize more connections to either Fabric or somewhere outside of EntERPSys_A specifically.
But that's hypothetical. We don't need to talk about it. It's kind of a little out of scope for us right now.

Participant_D   1:36:39
And then, yeah.

Participant_A   1:36:47
OK.
OK.

Participant_D   1:36:50
And then like so integration to external providers, right? Like are these, you know, are are these wired up as do you externalize authentication to like an entre tenant or are they separate creds?
You know service principles that that are used or like persistent creds.
To access to to do some to interface with their their services.

Participant_E   1:37:21
Participant_R may have to talk a little bit about that. As far as I know, it's only like SFTP drop a file and then ingest it and land it into EntERPSys_A, but I know they were talking about API access or more.
Uh.

Participant_R   1:37:37
Yeah.

Participant_E   1:37:39
Sorry, Participant_R.

Participant_R   1:37:41
No, I was going to say for VendorK we are using SFTP generating exports and then putting them out on the the SFTP. For GEP there is a an API like a custom solution that that utilizes APIs to push the the data to and from GEP to EntERPSys_A.

Participant_D   1:38:00
OK. And then on the connection side is, is, is the, are you guys hosting the SFTP instance and they're pulling it or are you pushing it out to an SFTP target on their side?

Participant_A   1:38:02
OK.

Participant_R   1:38:13
I'd have to double check with Andrew. He's probably the person that would know that. Oh, that's the best second thing I hear in the background.

Participant_D   1:38:17
OK.
Nice.

Participant_E   1:38:20
Best guess is we're not hosting that SFTP. They they host the SFTP account and we just have access creds to it.

Participant_D   1:38:25
OK.
OK.

Participant_P   1:38:33
And I don't know if this helps, but we do send data to other SFTP servers that that is not just VendorK and the authentication method is just password and and username. That's it.

Participant_D   1:38:48
OK.

Participant_A   1:38:51
How sensitive is that data that you're pushing out?

Participant_P   1:38:56
Transfer orders.

Participant_A   1:39:04
So I've worked with, for example, medical, you know, medical related data and PII and PHI and in those circumstances we've done.
SFTP, you know.
Granted access today through SFTP, but it's been because of the nature of the sensitivity of the data and the fact that you know if it got in the wrong hands kind of thing, there could be severe financial.
Consequences to it. We would also PGP encrypt that data at rest. But I think that's, you know, a question of for you is like how?
How sensitive is that that data?

Participant_D   1:39:57
Yeah, yeah. Or one way to like if the if the data got in the wrong hands of somebody else, if that, you know, could have that, would that happen? Would that create a negative impact to the business, right, is kind of what he's asking.

Participant_P   1:40:13
Hey.
I think the department in charge of that is probably the better.
Judge for that, but.

Participant_E   1:40:29
Yeah, it's, yeah, it's a hard question. There is no PII in there. Absolutely not. But like there is a numParticipant_K representation of a product movements, for example.
But let's say between the customer and the customer, those customer numbers are specific to our business.

Participant_D   1:40:52
OK. And then OK.

Participant_E   1:40:53
Right. So you you can't interpret the information without knowing who the customers are and that's not being sent. But there is like volumes for example like 10 units of this product is moving from here to here.
As an example, so there is no personas, no specificity to it, but and and a lot of it. Mm-hmm. Yep.

Participant_D   1:41:17
Mhm.

Participant_A   1:41:17
Well, let me ask the question this way.
Could access to this data potentially move markets?

Participant_E   1:41:33
I don't think so, but I'm not qualified to answer that question.
I don't know, Participant_R. Participant_R is here to answer any hard questions.

Participant_C   1:41:43
Participant_R or Participant_G? Or maybe Participant_G? Maybe Participant_G?

Participant_D   1:41:46
Mhm.

Participant_R   1:41:47
Yeah.

Participant_G   1:41:51
Sorry, what's the question?

Participant_A   1:41:54
I I wanted to know.
So that, you know, there was no like PII or that kind of sensitive data, but there are like orders, transfer orders, movement of product. You know, the question for me is like Chicago Mercantile Exchange, for example, would access to this kind of data potentially?
They move markets.

Participant_G   1:42:21
No.

Participant_A   1:42:23
OK.

Participant_G   1:42:24
No, no, no, no, no. You're talking about store transfers or any other kinds of transfers that wouldn't.
Change anything?

Participant_A   1:42:34
OK.
A big difference between oranges and potatoes.

Participant_G   1:42:39
Yeah, yeah, yeah.

Participant_D   1:42:41
Yeah.
Maybe this might be a question for Participant_M. Like does ACME have like data security policies that Acme-Sub needs to be governed through or complied by or do you know is it kind of you guys own that data?
And are responsible for it.

Participant_M   1:43:01
Yeah, I'd say outside of customer data and PII, that's been the big focus as far as like formalized policies to govern other classifications of data outside of that. I mean from a regulatory standpoint that has been kind of the focus.

Participant_D   1:43:02
Like.

Participant_M   1:43:20
Guess I would say.

Participant_D   1:43:21
OK.
OK.

Participant_E   1:43:28
So um.
As a question, what are you guys thinking? Because it's it's interesting we we stopped here and kind of been around this topic. I'm just curious, what are you, what are you guys have in mind as a workshop? Because we'd love to learn what you guys think.

Participant_D   1:43:35
It.
What?

Participant_E   1:43:47
Of certain things and how that can be implemented. Let's say if our vendor, the only means of communication is API, like how does that play in in or SFTP or whatever, right?

Participant_D   1:44:02
Yeah.

Participant_E   1:44:03
What are you guys thinking about right now?

Participant_D   1:44:07
Yeah, from a security perspective, it's always, you know, if if the data gets in the wrong hands, like a bad actor's hands, right, you know what is the impact of the business, right. And then you know the integration, there's always recommendations. So if it's API, right, then you know what, what does the API support, you know if it's limited to.
To like I I've done CloudDB_A integration and you know I think at the time you know it was limited at at you know client secret or if not it was it was it was probably Kafka. It was it was more older and the authentication method that they supported was very limited right.
So that was, you know, limited what is available to, you know, compared to modern authentication protocols like OIDC or anything like that. So from a security perspective, it gets surfaced up to security and then you know, we we we call out the risks of, you know, using this legacy off protocol.
Call for API integration. Is this acceptable for the business? But the objective is to get make sure that security is aware of it and it calls out to the business that OK, you know because of you know with this particular integration where data is flowing outside the organization, this is the limit.
Of of you know of what we could do to support authentication. Here's the risks. Is the business good with that, right? If it's sensitive data, right, then the you know if that if that credential gets popped.
You know, you know on the on the partner side, right, like what is that, you know, what is that input, what is that impact to your business, right? It it's really just to kind of broaden the conversation of of risk and you know what could happen, right?

Participant_E   1:45:59
Yeah, no, that's that's awesome. And I know Participant_R's been doing some work on our side from integration standpoint into looking at the vendors who do support more modern ways of transferring data or off.
For APIs or things like that, I know we we do talk about it, but some of our vendors we've been with are forever old and kind of stuck in the in the ways of.

Participant_D   1:46:26
Yeah, yeah.

Participant_E   1:46:32
Yeah, just the ways that they've they've done it for 20 years and that's the only way they do it. And I know Participant_R can kind of talk to that, but I know that Participant_R does bring it up during the vendor selection process.

Participant_D   1:46:35
Yeah, I get that.
Mhm.

Participant_E   1:46:48
That we we should move to more modern means of communication and encryption and other things using APIs instead of file transfers and what kind of auth APIs have and all of that.

Participant_A   1:47:02
Well, I I think it's a.

Participant_D   1:47:02
Yeah. And that and that's a, yeah, it's it's kind of like a, you know, hopefully all the stars align, right. But you know, you know, you know, sometimes it does it just like you said, you know, it depends on the vendor, right. At that point, you know, you know, that's why we make recommendations if you have to use username, password, OK.

Participant_E   1:47:10
Yep.

Participant_D   1:47:22
Like, can we create a policy to rotate that? How burdensome is it, right? Is it gonna, is it gonna, is it gonna add a certain level of protection for the business? Yes. But it's gonna have like, you know, you know, is that operational process cumbersome to IT? Are they gonna fall? Are they gonna?
You know what is available and how can we, how can we put the the appropriate mitigation to to get protection, right. And then to protect that that scenario and then obviously you know as we dive into it, right, you know, you know.
In the case of an incident, right, you know sometimes we have to build like you know attack timelines, right? So let's say worst case, you know VendorK was compromised. It wasn't out of your wasn't anything you guys did, but you know the result was VendorK, you know leaked customer data which happens to be your customer, right. I'm sure.
The ACME from the ACME side, you know, when they get involved from a security perspective, they're gonna wanna, you know, be able to track the the audit logs to see, you know, when was the initial foothold or like what happened, right? You know, for you know.
You know that led up to that that that incident, right. So in those specific cases it might be OK when when we do have to do a data exchange, let's let's make sure we're we're capturing that that activity. So from the SOC perspective we can you know reconstruct what happened.
Right. So that might just be information we wanna capture, right?

Participant_A   1:48:52
Yeah, it's also.
It's also true that even if you're doing SFTP and they're hosting, then you don't have access logs.
Of who accessed it.

Participant_E   1:49:06
Mhm.

Participant_D   1:49:07
Yeah. Or it might be one of those things that OK, can access. You gotta like there might be a question out to them. Hey, you guys gotta provide us some logging and then you know, you know the question to them would be like, you know, hey, I want, you know, I wanna make sure you guys, you know, because they may send you all the logs and then you know, hey, we want to be able to.
To a pinhole or create a VIP watch list like within Sims, we usually have VIP watch lists where we have specific alerts that are that are targeting specific individuals like executive at the executive level of the business, right.
Because those are prime targets, right? So you know this might be a service account or whatever credential that we want to make sure that any deviation from from this account or use from this account is you know.
Couldn't trigger an alert 'cause it'll instantiate the investigation, which you know really what we're doing is you have the appropriate protections like those are the protection. You know there's there's 33 kind of approaches we have to secure, protect, detect and respond, right? So obviously you you you.
Build the appropriate guardrails to protect the resource itself. And then can we increase the ability for detection, right? That's the ability for us to like notify the SOC or have, you know, instantiate an investigation to see if you know like what's going on.
And then if we can increase our detection, we can respond quicker, which means isolating the threat, right? So we don't get into scenario where you know it's, it's, you know, it's bad, it's a bad day for everybody, right?

Participant_A   1:50:53
Now Participant_D, I think also aren't there some mitigations you can do even if you're doing SFTP like you can force it so that the only it has to come from a pinned IP like which means like this this machine.

Participant_D   1:51:06
Yeah, you could pinhole it too, yeah.

Participant_A   1:51:11
This machine here on this client is the only one that can can submit or access you know this SFTP share.

Participant_D   1:51:21
Yeah.

Participant_A   1:51:22
Yeah, or using like a certificate based authorization authorization there.

Participant_D   1:51:25
Yeah, yeah, you're right. So there are different there, there are a lot of guard, you know, mitigations we could place, you know, very similar to this M4 firewall. I'm sure that M4 firewall between these two environments is there for a specific reason, right? And you know, the firewall is probably.
Pinholing communications between these two, just from authorized IPS, et cetera. You know, whatever it's doing, you could, you could do the same thing on this side, but we would look into it, right?

Participant_A   1:51:59
Yeah, I think there's also like like you could go with APIs, but that exposes a new surface area, for example.
There's a level of maturity, like do you need it? Like APIs are great for automation, but it's an open question. Like how often does the data go out? What is the process anyway? Like do you just have somebody who basically?

Participant_D   1:52:21
Yeah.

Participant_A   1:52:25
Grabs a file and SFT PS it somewhere and that's just part of your ordinary process and they only do it once a day. Or is it something that's going to run 24/7? You know these kinds of questions and you have limited resources and and and people and time, so you have to.

Participant_D   1:52:35
Yeah.

Participant_A   1:52:42
Decide, well, what do I have to prioritize? What's our, you know, what big, what's the what are the big fish we need to, you know, to deal with and then like, OK, we can mitigate this in another way and we don't have to deal with it right now. We can deal with that later if it becomes more of an issue, but right now.
It's kind of working fine and we just do some security mitigations to kind of make it less of a big deal. Um.

Participant_D   1:53:04
Yeah.

Participant_A   1:53:09
Just need to be mindful of your own bandwidth and what your own business priorities are. What's the cost of, you know, because it could be expensive to build that whole API and then you got to go through a vendor change. And I don't, I don't want to suggest that like, oh, you have to be, you know, one way or the other.

Participant_D   1:53:11
Yeah, yeah.
Yeah, well, yeah.
And unfortunately, yes, no, you're you're 100% right. And and unfortunately there isn't a silver bullet to this, right? You know, you we could just talk, you know what we look at is, you know, OK, here's in a specific scenario, how can it be exploited, right?
You know, that's where we we use threat modelling so we can identify you know potential areas that or or you know methods that can be exploited so we can come up with those mitigations. Like I said, there's no silver bullet, right? ADF is a good example where.
Some of those connectors, right? I think the CloudDB_A connector when you're when you're using ADF, it requires that you know a specific username and and password for it to connect to CloudDB_A because that's a persistent connection now.
Right. You know, like and it's and I think from the last time I did CloudDB_A, it was like a full user identity, right? So you know the problem that we have there is now we have to create an exception for that specific user identity.

Participant_A   1:54:19
Yes.

Participant_D   1:54:30
That's being used. We have to make sure it's it's very restricted and then now we have to to come up with like how do we rotate the password, right. And that could be an automated job which you know is is, you know, allows you to rotate the password, but then somebody still has to go to that connector and update it, right? Update the update.
The value of the new credit. So like I said, just to set expectations, it's it's, you know, there isn't a silver bullet. It's just making sure that we do our due diligence to look at, you know, understand all the integrations to know the overall solution to identify.
Areas that can be exploited, right. So that's what's driving a lot of kind of my definitely my questions of you know why I kind of like start focusing in specific areas when I when I hear things, but yeah.

Participant_A   1:55:20
Yeah, yeah. Oh, just just thinking out loud. You know you get into like SFTB like, yeah, you could do that with Data Factory. Of course, if you're using certificates or passwords, you know then you want to use using AKS to start your key or your.

Participant_D   1:55:34
Yeah, no.

Participant_A   1:55:36
Or your your credential or whatever that looks like and using that to perform it so so you can automate it that way and that's not a heavy lift to do. It's not.

Participant_D   1:55:46
Yeah, well, yeah, we we we love certificates from a security perspective. The problem you have with certificates now you need a now you need to govern them, right. You have a it's a whole entire infrastructure, PKI infrastructure, right. That that that needs to manage those, right. Which then.
Right. You know, can you use service principles and and shared secrets? Right now we run the same problem. How do you rotate those secrets on an ideal situation? You can use managed identities where you don't need to, where you need to.

Participant_M   1:56:11
But.

Participant_D   1:56:20
Where you don't need to manage the credential anymore, but you'll see where the the, you know, kind of the evolution of of software where they're going now is more towards Federated workload identities. So you know the idea there is is you know.

Participant_R   1:56:33
Yes.

Participant_D   1:56:36
You know, you're not managing credentials anymore. You just manage the authorization that that identity is being used for.

Participant_R   1:56:43
I know we're coming up on time and I think we're getting into more security discussions where we might not have the full audience for for that. Yeah, we'll be be sure to call out that we need the the security audience when we get into those topics again. But yeah, Yep.

Participant_D   1:56:49
Yep.
Yeah.
I didn't, you know. Oh, it's fine.

Participant_C   1:56:59
Hey guys, before I let you guys go, I just have one question. But again, Participant_D, if you're not done, that's fine.

Participant_D   1:57:05
No, no, I'm good. I I think, you know, just to summarize really quick, Participant_R, you know, I think you the diagram I showed earlier, that's something that Participant_A and I are gonna kind of work on. I'll be able to upload it for your guys's review and the securities review. I'll call it out like who who needs to review it.
You know on the team site, on your site.

Participant_R   1:57:24
Yeah, that would be perfect, Participant_D. Thank you.

Participant_D   1:57:27
OK.

Participant_C   1:57:27
Perfect. So, so Participant_P, just a quick question, right. So I I saw those six layers. I'm assuming that you know you have a thought process in Lakehouse about having you know maybe like tables for sensors such as like layer zero and one, maybe like historian and MES layer two and three and then the.
The ERP and business systems that you have four and five, I believe, is that what the hard process is when you're going to do the ingestion or that's what you're doing? Do you have any any, you know, schema for the database that you guys have created and how you are ingesting?

Participant_P   1:58:07
Yeah, so the UNS has some sort of schema where it breaks it down by site, then line, and then you can keep going down the layer.
So that's kinda how it's broken down.

Participant_C   1:58:27
OK, OK. The the reason I'm asking, right, so at the end of the day, right, when you are doing any kind of unified analytics, right or or maybe some real time, you know analysis or maybe time series joins, I I just want to understand the schema as well. I'm sure Participant_A wants to do that too. So you know we can.
Provide any kind of, you know, guidance or help there as well.

Participant_P   1:58:52
Yep, and and it did showed on our previous presentation like a sample of how that data structured with the topic timestamp and value. And so you can reference that and if you have any other questions I'm more than happy to.

Participant_C   1:58:58
OK.
OK.
OK.

Participant_P   1:59:11
Answer those, but yeah, in in the video you'll see you know the site, the line and then like all the different details that each value is associated with and that structure is built by our OT guys, so.

Participant_C   1:59:28
Sounds good. Thank you.

Participant_A   1:59:28
Yeah, I think, I think the the payload is really in the body section. I can't remember the the node and that the Jason of that of that OT data message that you sampled sent us a sample of. But there's, you know, time stamp that's just like.

Participant_P   1:59:30
Yep.

Participant_A   1:59:47
I've seen this kind of stuff coming out of BLOB stores, for example. It's the timestamp, these metadata, but then then you've got your actual payload, which is in like it's been encoded and it's in a.
So the and that can be very custom, right? So I think the the schema if you will, is that what is the schema that will be in the payload part of that?

Participant_P   2:00:13
Yeah.

Participant_C   2:00:17
Awesome. I think we have one topic left which was monitoring. I I think what I heard was a lot is already being done, but I, you know, we can definitely in our next workshop we can set aside some time just for monitoring, OK.
So that one topic was left today. We'll pick it up in the next workshop.
All right. I guess that's it for now. Thank you so much, everyone. Have a wonderful rest of the day. Take care. Thank you.

Participant_R   2:00:42
Sounds good.

Participant_P   2:00:47
Thanks everyone. Awesome. Thanks everybody.

Participant_A   2:00:48
Thanks guys.

Participant_B   2:00:48
That's thanks everyone.

Participant_D   2:00:52
Thank you guys. Bye.

Participant_B stopped transcription